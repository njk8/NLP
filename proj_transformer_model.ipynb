{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "proj_Transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/njk8/024864eca950edea8d0839b48c009baa/proj_transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75zOl8Iniwyk",
        "outputId": "b1b20bec-cbf0-4856-cbfc-0e21748b8cf3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9VUMWh3jTq_",
        "outputId": "e0657aef-e21a-49ae-962f-3c893c40e94f"
      },
      "source": [
        "%cd /content/drive/My Drive/studies/NLP/project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/studies/NLP/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha26xs8_mWVn"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPgoJcbimb8A"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjm6oqgG4nP"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkLrU_TPjcYn"
      },
      "source": [
        "with open(\"europarl-v7.sv-en.en\", mode='r', encoding='utf-8') as f:\n",
        "  europarl_en = f.read()\n",
        "\n",
        "with open(\"europarl-v7.sv-en.sv\", mode='r', encoding='utf-8') as f:\n",
        "  europarl_sv = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sc8SaMSGlKMB",
        "outputId": "06636b11-d313-43e9-a4f6-1ee6ea7478d3"
      },
      "source": [
        "europarl_en[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the se'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnFSFoGIiOSv"
      },
      "source": [
        "#a.m = a.$$$m = am"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B222UAT_kjOx"
      },
      "source": [
        "corpus_en = europarl_en\n",
        "#any char following '.' replace it with '.$$$'\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "#now we remove all such instance of '.$$$' from corpus\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "#replace two whitespaces with single whitespace \n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "#replace brackets with single whitespace \n",
        "corpus_en = re.sub(r\"\\(\", \"\", corpus_en)\n",
        "corpus_en = re.sub(r\"\\)\", \"\", corpus_en)\n",
        "\n",
        "#split each sentence in corpus based on '\\n' new line char\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_sv = europarl_sv\n",
        "corpus_sv = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_sv)\n",
        "corpus_sv = re.sub(r\".\\$\\$\\$\", '', corpus_sv)\n",
        "corpus_sv = re.sub(r\"  +\", \" \", corpus_sv)\n",
        "corpus_sv = re.sub(r\"\\(\", \"\", corpus_sv)\n",
        "corpus_sv = re.sub(r\"\\)\", \"\", corpus_sv)\n",
        "\n",
        "corpus_sv = corpus_sv.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgRaVqNBFKT8"
      },
      "source": [
        "#[[start, I, H]] 3 = 1x3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOVFdOmzmJNZ",
        "outputId": "cdcf4d43-382e-4dee-8638-ea0cf302bb90"
      },
      "source": [
        "corpus_en[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Resumption of the session',\n",
              " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
              " \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
              " 'You have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
              " \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\",\n",
              " \"Please rise, then, for this minute' s silence.\",\n",
              " \"The House rose and observed a minute' s silence\",\n",
              " 'Madam President, on a point of order.',\n",
              " 'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.',\n",
              " 'One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.',\n",
              " \"Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\",\n",
              " 'Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.',\n",
              " 'If the House agrees, I shall do as Mr Evans has suggested.',\n",
              " 'Madam President, on a point of order.',\n",
              " 'I would like your advice about Rule 143 concerning inadmissibility.',\n",
              " 'My question relates to something that will come up on Thursday and which I will then raise again.',\n",
              " 'The Cunha report on multiannual guidance programmes comes before Parliament on Thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually.',\n",
              " 'It says that this should be done despite the principle of relative stability.',\n",
              " 'I believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible.',\n",
              " 'I want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something I can competently do on Thursday.',\n",
              " 'That is precisely the time when you may, if you wish, raise this question, ie. on Thursday prior to the start of the presentation of the report.',\n",
              " '',\n",
              " \"Madam President, coinciding with this year' s first part-session of the European Parliament, a date has been set, unfortunately for next Thursday, in Texas in America, for the execution of a young 34 year-old man who has been sentenced to death. We shall call him Mr Hicks.\",\n",
              " 'At the request of a French Member, Mr Zimeray, a petition has already been presented, which many people signed, including myself. However, I would ask you, in accordance with the line which is now constantly followed by the European Parliament and by the whole of the European Community, to make representations, using the weight of your prestigious office and the institution you represent, to the President and to the Governor of Texas, Mr Bush, who has the power to order a stay of execution and to reprieve the condemned person.',\n",
              " 'This is all in accordance with the principles that we have always upheld.',\n",
              " 'Thank you, Mr Segni, I shall do so gladly.',\n",
              " 'Indeed, it is quite in keeping with the positions this House has always adopted.',\n",
              " 'Madam President, I should like to draw your attention to a case in which this Parliament has consistently shown an interest.',\n",
              " 'It is the case of Alexander Nikitin.',\n",
              " 'All of us here are pleased that the courts have acquitted him and made it clear that in Russia, too, access to environmental information is a constitutional right.',\n",
              " 'Now, however, he is to go before the courts once more because the public prosecutor is appealing.',\n",
              " 'We know, and we have stated as much in very many resolutions indeed, including specifically during the last plenary part-session of last year, that this is not solely a legal case and that it is wrong for Alexander Nikitin to be accused of criminal activity and treason because of our involvement as the beneficiaries of his findings.',\n",
              " \"These findings form the basis of the European programmes to protect the Barents Sea, and that is why I would ask you to examine a draft letter setting out the most important facts and to make Parliament's position, as expressed in the resolutions which it has adopted, clear as far as Russia is concerned.\",\n",
              " 'Yes, Mrs Schroedter, I shall be pleased to look into the facts of this case when I have received your letter.',\n",
              " 'Madam President, I would firstly like to compliment you on the fact that you have kept your word and that, during this first part-session of the new year, the number of television channels in our offices has indeed increased considerably.',\n",
              " 'But, Madam President, my personal request has not been met.',\n",
              " 'Although there are now two Finnish channels and one Portuguese one, there is still no Dutch channel, which is what I had requested because Dutch people here like to be able to follow the news too when we are sent to this place of exile every month.',\n",
              " 'I would therefore once more ask you to ensure that we get a Dutch channel as well.',\n",
              " \"Mrs Plooij-van Gorsel, I can tell you that this matter is on the agenda for the Quaestors' meeting on Wednesday.\",\n",
              " 'It will, I hope, be examined in a positive light.',\n",
              " 'Mrs Lynne, you are quite right and I shall check whether this has actually not been done.',\n",
              " 'I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.',\n",
              " 'Madam President, Mrs Díez González and I had tabled questions on certain opinions of the Vice-President, Mrs de Palacio, which appeared in a Spanish newspaper.',\n",
              " 'The competent services have not included them in the agenda on the grounds that they had been answered in a previous part-session.',\n",
              " 'I would ask that they reconsider, since this is not the case.',\n",
              " \"The questions answered previously referred to Mrs de Palacio' s intervention, on another occasion, and not to these comments which appeared in the ABC newspaper on 18 November.\",\n",
              " 'Mr Berenguer Fuster, we shall check all this.',\n",
              " 'I admit that, at present, the matter seems to be somewhat confused.',\n",
              " 'We shall therefore look into it properly to ensure that everything is as it should be.',\n",
              " 'In any event, this question is not presently included among the requests for topical and urgent debate on Thursday.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "URybEsnfWfuF",
        "outputId": "529b51cd-bf4d-40d0-d533-8467859e3f00"
      },
      "source": [
        "corpus_en[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDD33woF3Xs_"
      },
      "source": [
        "**Tokenizer will help us convert all sent to lower case, add spaces before '.' or ',' and also assign each word in the sentence with its unique integer value from its vocab.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqWzoUuKvSND"
      },
      "source": [
        "#Encoding is fully invertible because all out-of-vocab wordpieces are byte-encoded. \n",
        "#Which means unknown word pieces will be encoded one character at a time.\n",
        "#8219 + 26(all english alphabets)\n",
        "#target_vocab_size represents approx vocab size required!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl0XtAfqnMfR"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ZkN2gV4UBr"
      },
      "source": [
        "tokenizer_sv = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_sv, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAsKXHlcnu4K",
        "outputId": "4f21e98c-f612-407d-cdcd-4033d5d6eae4"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 \n",
        "VOCAB_SIZE_EN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uVPeUqDq5mm",
        "outputId": "22721eee-72c3-4ac4-87b4-00e2b64da7ee"
      },
      "source": [
        "tokenizer_sv.subwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['att_',\n",
              " ', ',\n",
              " 'och_',\n",
              " 'i_',\n",
              " 'som_',\n",
              " 'för_',\n",
              " 'en_',\n",
              " 'av_',\n",
              " 'det_',\n",
              " 'är_',\n",
              " 'de_',\n",
              " 'till_',\n",
              " 'om_',\n",
              " 'har_',\n",
              " 'på_',\n",
              " 'den_',\n",
              " 'med_',\n",
              " 'inte_',\n",
              " 'vi_',\n",
              " 's_',\n",
              " 'ett_',\n",
              " 'a_',\n",
              " 't_',\n",
              " 'Jag_',\n",
              " 'jag_',\n",
              " 'Det_',\n",
              " 'kommer_',\n",
              " 'kan_',\n",
              " 'måste_',\n",
              " 'detta_',\n",
              " 'r_',\n",
              " 'er_',\n",
              " 'från_',\n",
              " 'Vi_',\n",
              " 'n_',\n",
              " 'vill_',\n",
              " 'skulle_',\n",
              " 'också_',\n",
              " 'så_',\n",
              " 'na_',\n",
              " 'denna_',\n",
              " 'en',\n",
              " 'man_',\n",
              " 'EU',\n",
              " 'mycket_',\n",
              " ' - ',\n",
              " '. ',\n",
              " 'alla_',\n",
              " '! ',\n",
              " 'sig_',\n",
              " 'men_',\n",
              " 'när_',\n",
              " 'Europeiska_',\n",
              " 'vara_',\n",
              " 'eller_',\n",
              " 'talman',\n",
              " 'gäller_',\n",
              " 'Herr_',\n",
              " 'dessa_',\n",
              " 'andra_',\n",
              " 'kommissionen_',\n",
              " 'även_',\n",
              " 'mot_',\n",
              " 'I_',\n",
              " 'inom_',\n",
              " 'oss_',\n",
              " 'ska_',\n",
              " 'skall_',\n",
              " 'e_',\n",
              " 'finns_',\n",
              " 'utan_',\n",
              " 'under_',\n",
              " 'bara_',\n",
              " 'här_',\n",
              " 'et_',\n",
              " 'bör_',\n",
              " 'göra_',\n",
              " 'ta_',\n",
              " 'er',\n",
              " 'EU_',\n",
              " 'genom_',\n",
              " 'mer_',\n",
              " 'mellan_',\n",
              " ': ',\n",
              " 'var_',\n",
              " 'eftersom_',\n",
              " 'anser_',\n",
              " 'Detta_',\n",
              " 'na',\n",
              " 'nu_',\n",
              " 'kunna_',\n",
              " 'än_',\n",
              " 'vid_',\n",
              " 'vilket_',\n",
              " 'europeiska_',\n",
              " 'därför_',\n",
              " 'ar_',\n",
              " 'få_',\n",
              " 'nde_',\n",
              " 'där_',\n",
              " 'ni_',\n",
              " 'över_',\n",
              " 'ha_',\n",
              " 'fram_',\n",
              " 'Den_',\n",
              " 'allt_',\n",
              " 'våra_',\n",
              " 'nya_',\n",
              " 'För_',\n",
              " 'se_',\n",
              " 'fråga_',\n",
              " 'vad_',\n",
              " 'upp_',\n",
              " 'får_',\n",
              " '- ',\n",
              " 'vår_',\n",
              " 'mig_',\n",
              " 'rådet_',\n",
              " 'många_',\n",
              " 'sin_',\n",
              " 'åtgärder_',\n",
              " 'De_',\n",
              " 'hur_',\n",
              " 'särskilt_',\n",
              " 'första_',\n",
              " 'förslag_',\n",
              " 'parlamentet_',\n",
              " 'd_',\n",
              " 'as_',\n",
              " 'ts_',\n",
              " 'sina_',\n",
              " 'sätt_',\n",
              " 'något_',\n",
              " 'dem_',\n",
              " 'unionen_',\n",
              " 'redan_',\n",
              " 'år_',\n",
              " 'efter_',\n",
              " 'vilja_',\n",
              " 'ekonomiska_',\n",
              " 'säga_',\n",
              " 'viktigt_',\n",
              " 'politiska_',\n",
              " 'et',\n",
              " 'några_',\n",
              " 'helt_',\n",
              " 'ge_',\n",
              " 'min_',\n",
              " 'kommissionens_',\n",
              " 'frågor_',\n",
              " 'Men_',\n",
              " 'ande_',\n",
              " 'deras_',\n",
              " 'gemensamma_',\n",
              " 'ning_',\n",
              " 'två_',\n",
              " 'bli_',\n",
              " 'olika_',\n",
              " 'vissa_',\n",
              " 'stöd_',\n",
              " 'länder_',\n",
              " 'Om_',\n",
              " 'del_',\n",
              " 'gör_',\n",
              " 'Europa_',\n",
              " 'dag_',\n",
              " 'frågan_',\n",
              " 'det',\n",
              " 'betänkande_',\n",
              " 'hela_',\n",
              " 'herr_',\n",
              " 'behöver_',\n",
              " 'mänskliga_',\n",
              " 'sitt_',\n",
              " 'medlemsstaterna_',\n",
              " 'någon_',\n",
              " 'ens_',\n",
              " 'När_',\n",
              " 'ordförande',\n",
              " 'dess_',\n",
              " 'verkligen_',\n",
              " 'utskottet_',\n",
              " 'rätt_',\n",
              " 'dock_',\n",
              " 'då_',\n",
              " 'Därför_',\n",
              " 'han_',\n",
              " 'tror_',\n",
              " 'stora_',\n",
              " 'in_',\n",
              " 'mina_',\n",
              " ' – ',\n",
              " 'hade_',\n",
              " 'samma_',\n",
              " 'vårt_',\n",
              " 'ar',\n",
              " 'detta',\n",
              " 'hoppas_',\n",
              " 'fortfarande_',\n",
              " 'ändringsförslag_',\n",
              " 'blir_',\n",
              " 'sedan_',\n",
              " 'betänkandet_',\n",
              " 'Fru_',\n",
              " 'rättigheter_',\n",
              " 'stor_',\n",
              " 'vet_',\n",
              " 'Europa',\n",
              " 'nationella_',\n",
              " 'grund_',\n",
              " 'varit_',\n",
              " 'ser_',\n",
              " 'ger_',\n",
              " 'Europaparlamentet_',\n",
              " 'framför_',\n",
              " 'des_',\n",
              " 'ut_',\n",
              " 'unionens_',\n",
              " 'möjligt_',\n",
              " 'sociala_',\n",
              " 'ingen_',\n",
              " 'går_',\n",
              " 'problem_',\n",
              " 'rådets_',\n",
              " 'En_',\n",
              " 'står_',\n",
              " 'grundläggande_',\n",
              " 'innebär_',\n",
              " 'handlar_',\n",
              " 'om',\n",
              " 'enligt_',\n",
              " 'arbete_',\n",
              " 'erna_',\n",
              " 'procent_',\n",
              " 'ännu_',\n",
              " 'politik_',\n",
              " 'gå_',\n",
              " 're_',\n",
              " 'samt_',\n",
              " 'ing_',\n",
              " 'exempel_',\n",
              " 'år',\n",
              " 'rna_',\n",
              " ' ”',\n",
              " 'skapa_',\n",
              " 'vilka_',\n",
              " 'människor_',\n",
              " 'borde_',\n",
              " 'Som_',\n",
              " '. - ',\n",
              " 'större_',\n",
              " 'tar_',\n",
              " 'bättre_',\n",
              " 'inför_',\n",
              " 'tacka_',\n",
              " 'beslut_',\n",
              " 'både_',\n",
              " 'unionen',\n",
              " 'komma_',\n",
              " 'fall_',\n",
              " 'internationella_',\n",
              " 'sätt',\n",
              " 'Kommissionen_',\n",
              " 'bra_',\n",
              " 'viktiga_',\n",
              " 'liga_',\n",
              " 'ndet_',\n",
              " 'just_',\n",
              " 'da_',\n",
              " 'varje_',\n",
              " 'annat_',\n",
              " 'stödja_',\n",
              " 'senaste_',\n",
              " 'stöder_',\n",
              " 'kommissionen',\n",
              " 'iska_',\n",
              " 'medlemsstater_',\n",
              " 'herrar',\n",
              " 'mitt_',\n",
              " 'naturligtvis_',\n",
              " 'ytterligare_',\n",
              " 'nde',\n",
              " 'tid_',\n",
              " 'utveckling_',\n",
              " 'åt_',\n",
              " 'ts',\n",
              " 'samarbete_',\n",
              " 'emellertid_',\n",
              " 'land_',\n",
              " 'håller_',\n",
              " 'damer_',\n",
              " 'viktig_',\n",
              " 'förslaget_',\n",
              " 'punkt_',\n",
              " 'tre_',\n",
              " 're',\n",
              " 'företag_',\n",
              " 'het_',\n",
              " 'gjort_',\n",
              " 'as',\n",
              " 'dag',\n",
              " 'fråga',\n",
              " 'betänkande',\n",
              " 'nas_',\n",
              " 'roll_',\n",
              " 'an_',\n",
              " 'gång_',\n",
              " 'är',\n",
              " 'mål_',\n",
              " '\\xa0\\xa0 – ',\n",
              " 'fortsätta_',\n",
              " 'it_',\n",
              " 'till',\n",
              " 'själva_',\n",
              " 'ade_',\n",
              " 'parlamentets_',\n",
              " 'situationen_',\n",
              " 'europeisk_',\n",
              " 'ning',\n",
              " 'miljoner_',\n",
              " 'endast_',\n",
              " 'inre_',\n",
              " 'kanske_',\n",
              " 'avtal_',\n",
              " 'artikel_',\n",
              " 'lägga_',\n",
              " 'flera_',\n",
              " 'EN_',\n",
              " 'arna_',\n",
              " 'faktiskt_',\n",
              " 'rättsliga_',\n",
              " 'vilken_',\n",
              " '\\xa0',\n",
              " 'bland_',\n",
              " 'tydligt_',\n",
              " 'ligt_',\n",
              " 'visar_',\n",
              " 'hänsyn_',\n",
              " 'samtidigt_',\n",
              " 'nämligen_',\n",
              " 'alltid_',\n",
              " 'hans_',\n",
              " 'ns_',\n",
              " 'de',\n",
              " 'frågor',\n",
              " 'Låt_',\n",
              " 'o_',\n",
              " 'Förenta_',\n",
              " 'mindre_',\n",
              " 'enda_',\n",
              " 'tidigare_',\n",
              " 'system_',\n",
              " 'antal_',\n",
              " 'rättigheter',\n",
              " 'nuvarande_',\n",
              " 'området_',\n",
              " 'marknaden_',\n",
              " 'medborgare_',\n",
              " 'möjlighet_',\n",
              " 'ligger_',\n",
              " 'områden_',\n",
              " 'nödvändigt_',\n",
              " 'ny_',\n",
              " 'å_',\n",
              " 'an',\n",
              " 'länderna_',\n",
              " 'dem',\n",
              " 'era_',\n",
              " 'förbättra_',\n",
              " 'Vad_',\n",
              " 'för',\n",
              " 'direktiv_',\n",
              " 'ramen_',\n",
              " 'miljö',\n",
              " 'främja_',\n",
              " 'röstade_',\n",
              " 'ra_',\n",
              " 'emot_',\n",
              " 'het',\n",
              " 'hos_',\n",
              " 'Denna_',\n",
              " 'Med_',\n",
              " 'tredje_',\n",
              " 'gemensam_',\n",
              " 'ekonomisk_',\n",
              " 'utgör_',\n",
              " 'stort_',\n",
              " 'm_',\n",
              " 'små_',\n",
              " 'öka_',\n",
              " 'lika_',\n",
              " 'förslag',\n",
              " 'väl_',\n",
              " 'tanke_',\n",
              " 'al',\n",
              " 'På_',\n",
              " 'information_',\n",
              " 'medel_',\n",
              " 'på',\n",
              " 'energi',\n",
              " 'Att_',\n",
              " 'ningen_',\n",
              " 'välkomnar_',\n",
              " 'längre_',\n",
              " 'genomföra_',\n",
              " 'kvinnor_',\n",
              " 'stöd',\n",
              " 'sidan_',\n",
              " 'tillsammans_',\n",
              " 'grupp_',\n",
              " 'Man_',\n",
              " 'l_',\n",
              " 'den',\n",
              " 'g_',\n",
              " 'länder',\n",
              " 'tycker_',\n",
              " 'Ett_',\n",
              " 'här',\n",
              " 'trots_',\n",
              " 'samband_',\n",
              " 'använda_',\n",
              " 'k_',\n",
              " 'direktivet_',\n",
              " 'are_',\n",
              " 'vi',\n",
              " 'tt_',\n",
              " 'säger_',\n",
              " 'at_',\n",
              " 'ofta_',\n",
              " 'mest_',\n",
              " 'debatt_',\n",
              " 'alltså_',\n",
              " 'föredraganden_',\n",
              " 'parlamentet',\n",
              " 'garantera_',\n",
              " 'or',\n",
              " 'land',\n",
              " 'program_',\n",
              " 'fått_',\n",
              " '; ',\n",
              " 'personer_',\n",
              " 'talar_',\n",
              " 'medlemsstaterna',\n",
              " 'es_',\n",
              " 'närvarande_',\n",
              " 'kräver_',\n",
              " 'skriftlig',\n",
              " 'lig_',\n",
              " 'politisk_',\n",
              " 'nivå',\n",
              " 'Även_',\n",
              " '” ',\n",
              " 'Europaparlamentets_',\n",
              " 'ande',\n",
              " '1_',\n",
              " 'Dessa_',\n",
              " 'uppnå_',\n",
              " 'klart_',\n",
              " 'nivå_',\n",
              " 'annan_',\n",
              " 'minska_',\n",
              " 'rådet',\n",
              " 'största_',\n",
              " 'krävs_',\n",
              " 'debatten_',\n",
              " 'område_',\n",
              " 'val',\n",
              " 'ni',\n",
              " 'åtgärder',\n",
              " 'ka_',\n",
              " 'stället_',\n",
              " 'sådana_',\n",
              " 'rör_',\n",
              " 'kommissionär',\n",
              " 'utvecklingen_',\n",
              " 'delar_',\n",
              " 'rösta_',\n",
              " 'bidra_',\n",
              " 'or_',\n",
              " 'gruppen_',\n",
              " 'tala_',\n",
              " 'kunde_',\n",
              " 'med',\n",
              " 'Ni_',\n",
              " 'tas_',\n",
              " 'ansvar_',\n",
              " 'ningar_',\n",
              " 'st',\n",
              " 'offentliga_',\n",
              " 'ur_',\n",
              " 'egna_',\n",
              " 'synnerhet_',\n",
              " 'rättigheterna_',\n",
              " 'steg_',\n",
              " 'framsteg_',\n",
              " 'oss',\n",
              " 'sådan_',\n",
              " 'rum_',\n",
              " 'hjälpa_',\n",
              " 'problem',\n",
              " 'betänkandet',\n",
              " 'ordförandeskapet_',\n",
              " 'initiativ_',\n",
              " 'allmänna_',\n",
              " 'skydda_',\n",
              " 'ut',\n",
              " 'först_',\n",
              " 'behovet_',\n",
              " 'iga_',\n",
              " 'finansiella_',\n",
              " 'arbete',\n",
              " 'mål',\n",
              " 'av',\n",
              " 'strategi_',\n",
              " 'nings',\n",
              " 'tt',\n",
              " 'fru_',\n",
              " 'hjälp_',\n",
              " 'medlemsstaternas_',\n",
              " 'budget',\n",
              " 'tillgång_',\n",
              " 'nå_',\n",
              " 'leda_',\n",
              " 'heller_',\n",
              " 'kommissionsledamot',\n",
              " 'ord_',\n",
              " 'ordförande_',\n",
              " 'upp',\n",
              " 'området',\n",
              " 'känner_',\n",
              " 'visa_',\n",
              " 'dessutom_',\n",
              " 'utveckling',\n",
              " 'lösa_',\n",
              " 'medlemsstater',\n",
              " 'krav_',\n",
              " 'hålla_',\n",
              " 'att',\n",
              " 'ert_',\n",
              " 'säkerhet_',\n",
              " 'medborgarna_',\n",
              " 'ns',\n",
              " 'skydd_',\n",
              " 'börja_',\n",
              " 'nästa_',\n",
              " 'båda_',\n",
              " 'bästa_',\n",
              " 'avgörande_',\n",
              " 'arbeta_',\n",
              " 'politik',\n",
              " 'nu',\n",
              " 'euro_',\n",
              " 'viktigaste_',\n",
              " 'lagstiftning_',\n",
              " 'liksom_',\n",
              " 'kl',\n",
              " 'social_',\n",
              " 'möjligheter_',\n",
              " 'ing',\n",
              " 'Under_',\n",
              " 'ledamöter_',\n",
              " 'Och_',\n",
              " 'nytt_',\n",
              " 'svar_',\n",
              " 'Dessutom_',\n",
              " 'ändå_',\n",
              " 'all_',\n",
              " 'ra',\n",
              " 'regeringen_',\n",
              " 'avtalet_',\n",
              " 'li',\n",
              " '5_',\n",
              " 'enlighet_',\n",
              " 'fler_',\n",
              " 'ju_',\n",
              " 'införa_',\n",
              " 'bekämpa_',\n",
              " 'hög_',\n",
              " 'omfattande_',\n",
              " 'tjänster_',\n",
              " 'fiske',\n",
              " 'ag',\n",
              " 'lagt_',\n",
              " 'världen',\n",
              " 'am',\n",
              " 'uppmanar_',\n",
              " 'on_',\n",
              " 'ri',\n",
              " 'Europas_',\n",
              " 'kommande_',\n",
              " 'ber',\n",
              " 'marknaden',\n",
              " 'politiken_',\n",
              " 'st_',\n",
              " 'arbets',\n",
              " 'världen_',\n",
              " 'systemet_',\n",
              " 'före_',\n",
              " 'inte',\n",
              " 'ro',\n",
              " 'gemenskapens_',\n",
              " 'innan_',\n",
              " 'sagt_',\n",
              " 'demokratiska_',\n",
              " 'folk',\n",
              " 'ma_',\n",
              " 'framtida_',\n",
              " 'lag',\n",
              " 'faktum_',\n",
              " 'situation_',\n",
              " 'sett_',\n",
              " 'frågan',\n",
              " 'säkerhets',\n",
              " 'landet_',\n",
              " 'tagit_',\n",
              " 'snabbt_',\n",
              " 'te_',\n",
              " 'gen',\n",
              " 'konkurrens',\n",
              " 'staterna_',\n",
              " 'företag',\n",
              " 'egen_',\n",
              " 'område',\n",
              " 'lösning_',\n",
              " 'ja',\n",
              " 'förra_',\n",
              " 'Enligt_',\n",
              " 'instrument_',\n",
              " 'resolution_',\n",
              " 'icke',\n",
              " 'regler_',\n",
              " 'sk_',\n",
              " 'es',\n",
              " 'väl',\n",
              " 'FN',\n",
              " 'ed',\n",
              " '2_',\n",
              " 'el',\n",
              " 'tiden_',\n",
              " 'la',\n",
              " 'medelstora_',\n",
              " 'under',\n",
              " 'försöka_',\n",
              " 'fram',\n",
              " 'bestämmelser_',\n",
              " 'projekt_',\n",
              " 'slut',\n",
              " 'gräns',\n",
              " 'främst_',\n",
              " 'ram',\n",
              " 'resultat_',\n",
              " 'sade_',\n",
              " 'andra',\n",
              " 'ga_',\n",
              " 'blivit_',\n",
              " 'kolleger_',\n",
              " 'behov_',\n",
              " 'produkter_',\n",
              " 'syfte_',\n",
              " 'uppgifter_',\n",
              " 'klar',\n",
              " 'verkar_',\n",
              " 'le',\n",
              " 'form_',\n",
              " 'dra_',\n",
              " 'effektivt_',\n",
              " 'tillbaka_',\n",
              " 'ad_',\n",
              " 'stärka_',\n",
              " 'bet',\n",
              " 'vidta_',\n",
              " 'man',\n",
              " 'innehåller_',\n",
              " '000_',\n",
              " 'leder_',\n",
              " 'll',\n",
              " 'problemet_',\n",
              " 'ha',\n",
              " 'syftar_',\n",
              " 'rna',\n",
              " 'framtiden',\n",
              " 'utveckla_',\n",
              " 'särskilda_',\n",
              " 'in',\n",
              " 'la_',\n",
              " 'områden',\n",
              " 'ståndpunkt_',\n",
              " 'procent',\n",
              " 'tillräckligt_',\n",
              " '0_',\n",
              " 'och',\n",
              " 'sig',\n",
              " 'ng',\n",
              " 'forskning_',\n",
              " 'itu_',\n",
              " 'alltför_',\n",
              " 'gruppen',\n",
              " 'minst_',\n",
              " 'alla',\n",
              " 'nödvändiga_',\n",
              " '00',\n",
              " 'ligen_',\n",
              " 'visat_',\n",
              " 'genomförandet_',\n",
              " 'försöker_',\n",
              " 'at',\n",
              " 'go',\n",
              " 'da',\n",
              " '\" ',\n",
              " '3_',\n",
              " 'hälso',\n",
              " 'lo',\n",
              " 'gjorde_',\n",
              " 'gi',\n",
              " 'ta',\n",
              " 'finnas_',\n",
              " 'grund',\n",
              " 'direkt_',\n",
              " 'kampen_',\n",
              " 'såsom_',\n",
              " 'Till_',\n",
              " 'enbart_',\n",
              " 'fortsätter_',\n",
              " 'sådant_',\n",
              " 'inget_',\n",
              " 'fe',\n",
              " 'fullt_',\n",
              " 'vare_',\n",
              " 'tid',\n",
              " 'tänker_',\n",
              " 'villkor_',\n",
              " 'har',\n",
              " 'långt_',\n",
              " 'hon_',\n",
              " 'ber_',\n",
              " 'aldrig_',\n",
              " 'möjligt',\n",
              " 'Europaparlamentet',\n",
              " 'framtiden_',\n",
              " 'bidrag_',\n",
              " 'anta_',\n",
              " 'fick_',\n",
              " 'fullständigt_',\n",
              " 'do',\n",
              " 'on',\n",
              " 'gratulera_',\n",
              " 'betydelse_',\n",
              " 'Hur_',\n",
              " 'myndigheterna_',\n",
              " 'vidare_',\n",
              " 'ter',\n",
              " 'absolut_',\n",
              " 'förordning_',\n",
              " 'snart_',\n",
              " 'där',\n",
              " 'saker_',\n",
              " 'pe',\n",
              " 'kolleger',\n",
              " 'ena_',\n",
              " 'nyligen_',\n",
              " 'skäl_',\n",
              " 'barn_',\n",
              " 'enkelt_',\n",
              " 'därmed_',\n",
              " 'sista_',\n",
              " 'fast_',\n",
              " 'kort_',\n",
              " 'miss',\n",
              " 'avtal',\n",
              " 'över',\n",
              " 'Nu_',\n",
              " 'erna',\n",
              " 'rad_',\n",
              " 'haft_',\n",
              " 'resurser_',\n",
              " 'föra_',\n",
              " 'barn',\n",
              " 'v_',\n",
              " 'goda_',\n",
              " 'rätt',\n",
              " 'föreslår_',\n",
              " 'it',\n",
              " 'politiskt_',\n",
              " 'ter_',\n",
              " 'utanför_',\n",
              " 'ng_',\n",
              " 'rätta_',\n",
              " 'inga_',\n",
              " 'världs',\n",
              " 'viss_',\n",
              " 'varför_',\n",
              " 'behövs_',\n",
              " 'internationell_',\n",
              " 'di',\n",
              " 'betona_',\n",
              " 'y_',\n",
              " 'vore_',\n",
              " 'åtminstone_',\n",
              " 'som',\n",
              " 'andet_',\n",
              " '4_',\n",
              " 'programmet_',\n",
              " 'ings',\n",
              " 'mi',\n",
              " 'ad',\n",
              " 'ord',\n",
              " 'ko',\n",
              " 'lång_',\n",
              " 'hand_',\n",
              " 'ka',\n",
              " 'hållbar_',\n",
              " 'ul',\n",
              " 'Så_',\n",
              " 'fördraget_',\n",
              " 'länge_',\n",
              " 'flyg',\n",
              " 'ud',\n",
              " 'Ryssland_',\n",
              " 'nära_',\n",
              " 'Samtidigt_',\n",
              " 'arbetet_',\n",
              " 'vis_',\n",
              " 'väg_',\n",
              " 'ekonomiskt_',\n",
              " 'grad_',\n",
              " 'svårt_',\n",
              " 'pengar_',\n",
              " 'följd_',\n",
              " 'ma',\n",
              " 'kommit_',\n",
              " 'be',\n",
              " 'göra',\n",
              " 'arbetar_',\n",
              " 'fallet_',\n",
              " 'högre_',\n",
              " 'intresse_',\n",
              " 'are',\n",
              " 'tex',\n",
              " 'Eftersom_',\n",
              " 'transport',\n",
              " 'verksamhet_',\n",
              " 'ställa_',\n",
              " 'kollega_',\n",
              " 'medborgare',\n",
              " 'euro',\n",
              " 'tekniska_',\n",
              " 'stor',\n",
              " 'sam',\n",
              " 'avslutad',\n",
              " 'rättigheterna',\n",
              " 'Turkiet_',\n",
              " 'diskutera_',\n",
              " 'rätten_',\n",
              " 'vatten',\n",
              " 'fria_',\n",
              " 'mig',\n",
              " 'ets_',\n",
              " 'Tack_',\n",
              " 'För',\n",
              " 'överens_',\n",
              " 'så',\n",
              " 'kammaren_',\n",
              " '\\xa0\\xa0 . – ',\n",
              " 'såväl_',\n",
              " 'principen_',\n",
              " 'ck',\n",
              " 'bort_',\n",
              " 'vin',\n",
              " 'Le',\n",
              " 'ske_',\n",
              " 'ton',\n",
              " 'låta_',\n",
              " 'handel_',\n",
              " 'språk',\n",
              " 'um',\n",
              " 'z_',\n",
              " 'insatser_',\n",
              " 'vars_',\n",
              " 'precis_',\n",
              " 'DE_',\n",
              " 'globala_',\n",
              " 'regionala_',\n",
              " 'kontroll',\n",
              " 'ki',\n",
              " 'Slutligen_',\n",
              " 'va_',\n",
              " 'fall',\n",
              " 'é',\n",
              " 'be_',\n",
              " 'förhandlingarna_',\n",
              " 'svar',\n",
              " 'processen_',\n",
              " 'sak',\n",
              " 'undvika_',\n",
              " 'kontroll_',\n",
              " 'samarbetet_',\n",
              " 'handels',\n",
              " '”.',\n",
              " 'utbildning_',\n",
              " 'Nästa_',\n",
              " 'enskilda_',\n",
              " 'antalet_',\n",
              " 'intresse',\n",
              " 'tyvärr_',\n",
              " 'Ma',\n",
              " 'dagens_',\n",
              " 'början_',\n",
              " 'Genom_',\n",
              " 'nästan_',\n",
              " 'följa_',\n",
              " '6_',\n",
              " 'avseende_',\n",
              " 'ten_',\n",
              " 'bi',\n",
              " 'beroende_',\n",
              " 'sför',\n",
              " 'ja_',\n",
              " 'beslut',\n",
              " 'själv_',\n",
              " 'åter',\n",
              " 'sta',\n",
              " 'oberoende_',\n",
              " 'system',\n",
              " 'is',\n",
              " 'Trots_',\n",
              " 'Parlamentet_',\n",
              " 'stats',\n",
              " 'intressen',\n",
              " 'slutet_',\n",
              " 'fem_',\n",
              " 'tids',\n",
              " 'krisen_',\n",
              " 'ir',\n",
              " 'nationell_',\n",
              " 'heten_',\n",
              " 'tra',\n",
              " 'kommissionsledamoten',\n",
              " 'tro',\n",
              " 'su',\n",
              " 'el_',\n",
              " 'sker_',\n",
              " 'gemensamt_',\n",
              " 'Ka',\n",
              " 'efter',\n",
              " 'konkreta_',\n",
              " 'Alla_',\n",
              " 'betydande_',\n",
              " 'Vi',\n",
              " 'allvarliga_',\n",
              " 'äga_',\n",
              " 'förhindra_',\n",
              " 'ned',\n",
              " 'exempelvis_',\n",
              " 'fri',\n",
              " 'rar_',\n",
              " 'In',\n",
              " 'budget_',\n",
              " 'bygga_',\n",
              " 'fyra_',\n",
              " 'positiva_',\n",
              " 'Ta',\n",
              " 'sektorn_',\n",
              " 'jordbruks',\n",
              " 'fin',\n",
              " 'kunnat_',\n",
              " 'to',\n",
              " 'tre',\n",
              " 'sätta_',\n",
              " 'ver',\n",
              " 'dvs',\n",
              " 'europeiskt_',\n",
              " 'själv',\n",
              " 'bakom_',\n",
              " 'ci',\n",
              " 'du',\n",
              " 'program',\n",
              " 'tack_',\n",
              " 'ot',\n",
              " 'menar_',\n",
              " 'gemenskaps',\n",
              " 'medan_',\n",
              " 'dialog_',\n",
              " 'snarare_',\n",
              " 'helst_',\n",
              " ' \" ',\n",
              " 'åt',\n",
              " 'var',\n",
              " 'lokala_',\n",
              " 'demokrati',\n",
              " 'berörda_',\n",
              " 'hjälp',\n",
              " 'Sa',\n",
              " 'möte_',\n",
              " 'Min_',\n",
              " 'änd',\n",
              " 'respekt_',\n",
              " 'miljarder_',\n",
              " 'eri',\n",
              " 'befinner_',\n",
              " 'pre',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlWiT8KlcB3r",
        "outputId": "4d382b5a-812e-4135-fdb4-7b59a0d79a12"
      },
      "source": [
        "VOCAB_SIZE_SV = tokenizer_sv.vocab_size + 2\n",
        "VOCAB_SIZE_SV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHC0Cz_qmdy"
      },
      "source": [
        "**Pad the 'start' and 'end' token to all sentences in the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sro1ih4ZuBh-"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] for sentence in corpus_en]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq8lN013cVKC"
      },
      "source": [
        "outputs = [[VOCAB_SIZE_SV-2] + tokenizer_sv.encode(sentence) + [VOCAB_SIZE_SV-1] for sentence in corpus_sv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GCBnbnGvJF0",
        "outputId": "eab0315c-2600-4add-a218-98f5e9375c2e"
      },
      "source": [
        "inputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8215, 2562, 1015, 2030, 3, 1, 2578, 8216]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QhSZMz_EvS6Z",
        "outputId": "0ff3bb7e-e9e3-4fd5-9bf4-8a8a7c0f414f"
      },
      "source": [
        "corpus_en[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DsMEoZacn4g",
        "outputId": "0600cbfd-f37c-48a2-d90c-9820285cb978"
      },
      "source": [
        "outputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8182, 3362, 79, 7381, 5918, 8, 5877, 42, 8183]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGnqe-Ql6e5N"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\n",
        "\n",
        "#delete sentences from inputs(source) that exceed max len of 20, correspondingly also delete sentences in outputs(target).\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "#we do the same thing for outputs(target) ie: find sent that exceed max len of 20 in target outputs and del from both outputs \n",
        "#and inputs\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOftLKzA7Mxi"
      },
      "source": [
        "**Pad value of 0 for sentences less than its max length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyQuEzzd7h1G"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, padding='post', maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChAnCJU7sn4",
        "outputId": "ae16e478-072f-4c03-ea9b-6acadfe095c7"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(446931, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRSGbGfDeMCj"
      },
      "source": [
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, padding='post', maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoKGYd2Aen2Y",
        "outputId": "14f8340a-18ec-4d79-8eb2-aaf43c216180"
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(446931, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h54QSTWNna9",
        "outputId": "d5de1951-3b6f-4acc-e881-0f29cf87e4fb"
      },
      "source": [
        "valid_src = inputs[-1000:]\n",
        "valid_ref = outputs[-1000:]\n",
        "valid_src.shape, valid_ref.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 20), (1000, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdZXp64CcgP9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSw6oKpOOAaz",
        "outputId": "1e839331-cd77-4b52-8f2c-b3531ef8527a"
      },
      "source": [
        "inputs = inputs[:-1000]\n",
        "outputs = outputs[:-1000]\n",
        "inputs.shape, outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((445931, 20), (445931, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1t1E9jeu1R"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "#to help increase speed during training - store it in cache\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "#to help access to data faster - further improving speed (Note: it has no effect on accuracy)\n",
        "#This transformation basically uses a background thread and an internal buffer to prefetch elements \n",
        "#from the input dataset ahead of the time they are requested.\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YLnyluZsCmt",
        "outputId": "bb09a34e-56a3-45cd-b480-42e0ae773edc"
      },
      "source": [
        "next(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
              " array([[8215,   37,   17, ...,    0,    0,    0],\n",
              "        [8215,  579,   60, ...,    0,    0,    0],\n",
              "        [8215, 1057,  314, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8215, 1314,  183, ...,    0,    0,    0],\n",
              "        [8215,   67,    9, ...,    0,    0,    0],\n",
              "        [8215,  262, 7717, ...,    0,    0,    0]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
              " array([[8182,  411,   28, ...,    0,    0,    0],\n",
              "        [8182,   26,   70, ...,    0,    0,    0],\n",
              "        [8182,   26, 3088, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8182,  748,  815, ...,    0,    0,    0],\n",
              "        [8182,  926,   10, ...,    0,    0,    0],\n",
              "        [8182, 7450, 7958, ..., 1952, 7972, 8183]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PxIclTyBnP4"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        #pos - [seqlen, 1] list of all positions\n",
        "        #i - [1, d_model] list of all dimensions\n",
        "        #d_model - dimension size\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #get the first dimension of the input tensor (seq length)\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        #get the second dimension of the input tensor (embedding dim)\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        #send list of positions from 0 to seq length with an additional axis [seq, 1], send list of dimensions [1, dim]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "        #all angle values 0 to : with a step of 2 (to access even part)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        #1:all:2 to access the odd part\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # add extra dim [1, seq, d_model] for batch size [batch, seq, d_model]\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        #concat (or add the input to pos enc (convert to tensor))\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvT5ErCbp84U"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    #Q, K and V - size - [..., seq, emb_dim]\n",
        "    #matrix multiply query with the transpose of key matrix => [..., 20, 20]\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "    #get the keys dimension size, type caste to float\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "\n",
        "    #scale the product by keys dimension -> so as to get consistent variance regardless the value of dim of keys\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    #if there is a valid mask\n",
        "    if mask is not None:\n",
        "        #all values except zero will be multiplied by -1e9 in the mask which is further added to the scaled product.\n",
        "        #this makes sure that softmax is applied the padded values go to zeroes thus not affecting the original sent length. \n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    #print(scaled_product.shape)\n",
        "    #print(tf.nn.softmax(scaled_product, axis=-1).shape)\n",
        "    #print(values.shape)\n",
        "    \n",
        "    #finally we apply softmax along last dimension such that prob of seq sum up to 1, \n",
        "    #and we multiply the result with values matrix.\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)  # [batch, nb.proj, seq, nb.dim]\n",
        "    \n",
        "    return attention, tf.nn.softmax(scaled_product, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpPHdDlOkMbe",
        "outputId": "a4d98339-3643-481f-817c-f2bb862fb509"
      },
      "source": [
        "x = tf.random.uniform((64, 8, 20, 64))\n",
        "product, _ = scaled_dot_product_attention(x, x, x, mask=None)\n",
        "product.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 8, 20, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFdy_i_kvXc2",
        "outputId": "2186a8ae-3848-4fb1-cf25-6cb28c79b40a"
      },
      "source": [
        "[2, 4, 6, 0, 0 ,0 ] , [0, 0, 0 , 1, 1, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2, 4, 6, 0, 0, 0], [0, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsbPguu13Ha2"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        #call the base class\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        #initialize the no. of projections\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        #get the dimension (d_model)\n",
        "        self.d_model = input_shape[-1]\n",
        "        #print(input_shape[-1])\n",
        "\n",
        "        #we check if the d_model dimension is divisible by no. of proj\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        #divide and get only the integer part of the fraction\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        #define dense layers having d_model hidden units for Q, K and V\n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        #Final output linear layer\n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        #we define the shape we want of our output tensor here\n",
        "        #[Batch, seq, nb_proj, dim_proj]\n",
        "\n",
        "        shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        #Now we reshape the inputs into the above defined shape\n",
        "        \n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        #get the bactch size\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        #apply the layers onto Q, K and V\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        #we get the splitted projections for Q, K and V respectively\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        #get the attention weights\n",
        "        attention, weights = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        #permute and get back original tensor shape of [batch, seq, nb.proj, dim_proj]\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        #merge and reshape back into [Batch, seq, nb.proj * dim_proj] = [Batch, seq, d_model]\n",
        "        #print(attention[-1][-1].shape)\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "        #print(concat_attention.shape)\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)  #[Batch, Seq, d_model]\n",
        "\n",
        "        return outputs, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVsi1bOGZDu"
      },
      "source": [
        "temp_mha = MultiHeadAttention(8)\n",
        "y = tf.random.uniform((64, 20, 512))  # (batch_size, seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxrJ5t60G_fj",
        "outputId": "c33341fb-81da-4f98-a969-27a4d3c5f716"
      },
      "source": [
        "out, w = temp_mha(y, y, y, mask=None)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EGqYzIoKUnV"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        #call MHA here the key, query and value == input\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention, _ = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdQnf8aBBTQG",
        "outputId": "2a3307ee-f275-43d2-e10d-adcfe4f78dc2"
      },
      "source": [
        "EL = EncoderLayer(1024, 8, 0.1)\n",
        "x = tf.random.uniform((64, 20, 512))\n",
        "EL(x, None, False).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOnGMNsvZp_2"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size, d_model, name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        #create Encodinglayer for given nb_layers (eg:6) of time!\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate) for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5CjemZCrYm",
        "outputId": "0bdb3041-d92a-4a02-a5ad-ec57c4b939ff"
      },
      "source": [
        "enc = Encoder(6, 1024, 8, 0.1, 8192, 512)\n",
        "x = tf.random.uniform((64, 20)) #[Batch, Seq]\n",
        "enc(x, None, False).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCqKnuUAAkS6"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self Multi head attention with itself\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward Network\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        #apply MHA with padding mask\n",
        "        attention, wb1 = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        #apply MHA with look ahead mask\n",
        "        attention_2, wb2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        #apply the final FFN layer\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs, wb1, wb2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tilHdNz4Eji6"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size, d_model, name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout_rate) for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            #loop through and call all such decoder layer instances\n",
        "            outputs, wb1, wb2 = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
        "\n",
        "        return outputs, wb2 #[Batch, Seq, d_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFV25pKmFj9j"
      },
      "source": [
        "##**Now Lets! combine everything together to build our final transformer architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obdURUudF8w9"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout_rate, name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size_enc, d_model)\n",
        "        \n",
        "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size_dec, d_model)\n",
        "\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"final_output\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        #add extra 2 empty dimesions [Batch, nb.proj, seq, seq] to be used after scaled dot product [batch, nb.proj, seq, seq]\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        #here we only consider the lower left traingle and hide upper right traingle of the matrix\n",
        "        #-1 -> keep lower half and 0 -> disable upper traingle (using the linalg.band_part function)\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        #encoder mask\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        #decoder first mask \n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\n",
        "        #decoder second mask: here we use encoder inputs since our keys and values to MHA are from the output of encoder \n",
        "        #and queries come from decoder side. We want to mask encoder padded outputs when we recombine with decoder inputs.\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        #get the encoder outputs\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        #get the decoder outputs\n",
        "        dec_outputs, weights = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
        "        \n",
        "        #apply the final output layer of unit = decoder vocab size (such that the model will \n",
        "        #predict the words from swedish vocab that have high probability given english input sentence.)\n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs, weights  #[Batch, Seq, Voacab_size_dec]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQv-faWRfJvh",
        "outputId": "725fe000-2756-42c4-963a-fe3f9fa39348"
      },
      "source": [
        "tf.linalg.band_part(tf.ones((10, 10)), 0, -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUE83fuddTr"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSQx_6Y3eq9N"
      },
      "source": [
        "def create_look_ahead_mask(seq):\n",
        "  seq_len = tf.shape(seq)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  return look_ahead_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unCAQECYdqL6",
        "outputId": "94ac5160-2ce3-4972-e4d1-55afaec00239"
      },
      "source": [
        "seq = tf.cast([[583, 288, 0, 412, 103, 0, 0, 0]], tf.int32)\n",
        "create_padding_mask(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 8), dtype=float32, numpy=array([[[[0., 0., 1., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRQMdTSYewLq",
        "outputId": "2def09e3-f9d1-4a29-e92b-3e0e500e5a6e"
      },
      "source": [
        "create_look_ahead_mask(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\n",
              "array([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYSJ_j-ffxq4",
        "outputId": "c1339452-c61d-46eb-feaa-857b0619081e"
      },
      "source": [
        "#automatically reshapes into [..., seq, seq] and compares with look ahead max\n",
        "#this operation helps us apply both the mask!\n",
        "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 8, 8), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrbJu9K1n0pB"
      },
      "source": [
        "tm = Transformer(10000, 10000, 512, 6, 1024, 8, 0.1)\n",
        "t_input = tf.random.uniform((1, 20), dtype=tf.int64, minval=0, maxval=200)\n",
        "t_target = tf.random.uniform((1, 20), dtype=tf.int64, minval=0, maxval=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wBuoV3Po-rn",
        "outputId": "f96cb3b0-0d3a-40bb-b92b-d2d55f3e3b7a"
      },
      "source": [
        "a, w = tm(t_input, t_target, False)\n",
        "a.shape, w.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 20, 10000]), TensorShape([1, 8, 20, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrcHi57XR_J",
        "outputId": "717e7e1d-bed5-46ba-c758-da2774b10ec9"
      },
      "source": [
        "head = 0\n",
        "# shape: (batch=1, num_heads, seq_len_q, seq_len_k)\n",
        "attention_heads = tf.squeeze(w, 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([20, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crjPBv53feyn"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYmZN_JhQIln"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Lets define the Hyper-parameters needed to train our dataset\n",
        "# we initially choose small size parameter values for faster training compared to as stated in the paper!\n",
        "D_MODEL = 128       # 512\n",
        "NB_LAYERS = 4       # 6\n",
        "FFN_UNITS = 512     # 2048\n",
        "NB_PROJ = 8         # 8\n",
        "DROPOUT_RATE = 0.1  # 0.1\n",
        "\n",
        "#Instantiate the transformer model\n",
        "transformer = Transformer(VOCAB_SIZE_EN, VOCAB_SIZE_SV, D_MODEL, NB_LAYERS, FFN_UNITS, NB_PROJ, DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBDqGP9Roxn"
      },
      "source": [
        "**Now before we start training we need to do few very important steps:**<br><br>1) First we define our loss object as Sparse CategoricalCrossentropy (we use this crossentropy loss function since in the output we have two or more class labels to predict.)<br><br>2) Next we define loss function that creates a mask to hide the padded values and do not include it in the computaion of loss metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpSE_ri8RgxF"
      },
      "source": [
        "#since our outputs from model are real numbers ready to be transformed into probabilities we set from_logits = True.\n",
        "#And reduction none indicates dont sum over all probabilities and calc mean loss as of yet. Since we need to remove the \n",
        "#padding part before summing the loss!\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"training_loss\") #to keep track of training loss!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwv77A7hiwHc"
      },
      "source": [
        "**Now Instead using a fixed pre-set learning rate at all times during the training phase the research paper decides to use a custom learning rate, such that for initial 4000 steps the learning rate is fast (increases linearly) and thereafter it starts decreasing.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8qJzfgwhMzT"
      },
      "source": [
        "class CustomLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, start_steps=4000):\n",
        "        super(CustomLearningRate, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.start_steps = start_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.start_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "new_learning_rate = CustomLearningRate(D_MODEL)\n",
        "\n",
        "#we set the paramertes as given in the paper!\n",
        "optimizer = tf.keras.optimizers.Adam(new_learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-HD3Od6OnqZ"
      },
      "source": [
        "**create checkpoints path to save checkpoints after every epoch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl80zajnOiin"
      },
      "source": [
        "import sys\n",
        "\n",
        "checkpoint_path = \"./store/ckpt/10hepo\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f8j5QM4KA-x"
      },
      "source": [
        "#dec_inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo4VuqPQSeJn"
      },
      "source": [
        "**Its time to start training!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwV4yVEKPmOE",
        "outputId": "7a86809f-33d7-4901-bb97-534d4fc1e106"
      },
      "source": [
        "\n",
        "EPOCHS = 10  #5-6\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        #get everything except last word for input to the decoder\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        #and the output from decoder is the shifted right part.\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        \n",
        "\n",
        "        #store everything that happens during training on a tape\n",
        "        with tf.GradientTape() as tape:\n",
        "            #print('hello')\n",
        "            predictions, _ = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "            \n",
        "        \n",
        "        #Calc gradients dL/dw\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        #update the weights\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1, batch, train_loss.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.2952\n",
            "Epoch 1 Batch 50 Loss 5.6741\n",
            "Epoch 1 Batch 100 Loss 5.6392\n",
            "Epoch 1 Batch 150 Loss 5.5800\n",
            "Epoch 1 Batch 200 Loss 5.5215\n",
            "Epoch 1 Batch 250 Loss 5.4559\n",
            "Epoch 1 Batch 300 Loss 5.3572\n",
            "Epoch 1 Batch 350 Loss 5.2503\n",
            "Epoch 1 Batch 400 Loss 5.1466\n",
            "Epoch 1 Batch 450 Loss 5.0503\n",
            "Epoch 1 Batch 500 Loss 4.9566\n",
            "Epoch 1 Batch 550 Loss 4.8648\n",
            "Epoch 1 Batch 600 Loss 4.7858\n",
            "Epoch 1 Batch 650 Loss 4.7068\n",
            "Epoch 1 Batch 700 Loss 4.6354\n",
            "Epoch 1 Batch 750 Loss 4.5656\n",
            "Epoch 1 Batch 800 Loss 4.5015\n",
            "Epoch 1 Batch 850 Loss 4.4415\n",
            "Epoch 1 Batch 900 Loss 4.3879\n",
            "Epoch 1 Batch 950 Loss 4.3354\n",
            "Epoch 1 Batch 1000 Loss 4.2870\n",
            "Epoch 1 Batch 1050 Loss 4.2405\n",
            "Epoch 1 Batch 1100 Loss 4.1969\n",
            "Epoch 1 Batch 1150 Loss 4.1551\n",
            "Epoch 1 Batch 1200 Loss 4.1152\n",
            "Epoch 1 Batch 1250 Loss 4.0756\n",
            "Epoch 1 Batch 1300 Loss 4.0402\n",
            "Epoch 1 Batch 1350 Loss 4.0088\n",
            "Epoch 1 Batch 1400 Loss 3.9765\n",
            "Epoch 1 Batch 1450 Loss 3.9442\n",
            "Epoch 1 Batch 1500 Loss 3.9156\n",
            "Epoch 1 Batch 1550 Loss 3.8872\n",
            "Epoch 1 Batch 1600 Loss 3.8592\n",
            "Epoch 1 Batch 1650 Loss 3.8341\n",
            "Epoch 1 Batch 1700 Loss 3.8093\n",
            "Epoch 1 Batch 1750 Loss 3.7843\n",
            "Epoch 1 Batch 1800 Loss 3.7595\n",
            "Epoch 1 Batch 1850 Loss 3.7371\n",
            "Epoch 1 Batch 1900 Loss 3.7124\n",
            "Epoch 1 Batch 1950 Loss 3.6899\n",
            "Epoch 1 Batch 2000 Loss 3.6686\n",
            "Epoch 1 Batch 2050 Loss 3.6476\n",
            "Epoch 1 Batch 2100 Loss 3.6268\n",
            "Epoch 1 Batch 2150 Loss 3.6062\n",
            "Epoch 1 Batch 2200 Loss 3.5856\n",
            "Epoch 1 Batch 2250 Loss 3.5660\n",
            "Epoch 1 Batch 2300 Loss 3.5454\n",
            "Epoch 1 Batch 2350 Loss 3.5254\n",
            "Epoch 1 Batch 2400 Loss 3.5061\n",
            "Epoch 1 Batch 2450 Loss 3.4865\n",
            "Epoch 1 Batch 2500 Loss 3.4673\n",
            "Epoch 1 Batch 2550 Loss 3.4485\n",
            "Epoch 1 Batch 2600 Loss 3.4300\n",
            "Epoch 1 Batch 2650 Loss 3.4115\n",
            "Epoch 1 Batch 2700 Loss 3.3932\n",
            "Epoch 1 Batch 2750 Loss 3.3749\n",
            "Epoch 1 Batch 2800 Loss 3.3563\n",
            "Epoch 1 Batch 2850 Loss 3.3373\n",
            "Epoch 1 Batch 2900 Loss 3.3188\n",
            "Epoch 1 Batch 2950 Loss 3.3016\n",
            "Epoch 1 Batch 3000 Loss 3.2850\n",
            "Epoch 1 Batch 3050 Loss 3.2686\n",
            "Epoch 1 Batch 3100 Loss 3.2519\n",
            "Epoch 1 Batch 3150 Loss 3.2357\n",
            "Epoch 1 Batch 3200 Loss 3.2192\n",
            "Epoch 1 Batch 3250 Loss 3.2039\n",
            "Epoch 1 Batch 3300 Loss 3.1885\n",
            "Epoch 1 Batch 3350 Loss 3.1735\n",
            "Epoch 1 Batch 3400 Loss 3.1584\n",
            "Epoch 1 Batch 3450 Loss 3.1445\n",
            "Epoch 1 Batch 3500 Loss 3.1301\n",
            "Epoch 1 Batch 3550 Loss 3.1166\n",
            "Epoch 1 Batch 3600 Loss 3.1029\n",
            "Epoch 1 Batch 3650 Loss 3.0896\n",
            "Epoch 1 Batch 3700 Loss 3.0765\n",
            "Epoch 1 Batch 3750 Loss 3.0638\n",
            "Epoch 1 Batch 3800 Loss 3.0503\n",
            "Epoch 1 Batch 3850 Loss 3.0372\n",
            "Epoch 1 Batch 3900 Loss 3.0245\n",
            "Epoch 1 Batch 3950 Loss 3.0119\n",
            "Epoch 1 Batch 4000 Loss 2.9997\n",
            "Epoch 1 Batch 4050 Loss 2.9876\n",
            "Epoch 1 Batch 4100 Loss 2.9754\n",
            "Epoch 1 Batch 4150 Loss 2.9627\n",
            "Epoch 1 Batch 4200 Loss 2.9504\n",
            "Epoch 1 Batch 4250 Loss 2.9382\n",
            "Epoch 1 Batch 4300 Loss 2.9260\n",
            "Epoch 1 Batch 4350 Loss 2.9138\n",
            "Epoch 1 Batch 4400 Loss 2.9027\n",
            "Epoch 1 Batch 4450 Loss 2.8910\n",
            "Epoch 1 Batch 4500 Loss 2.8795\n",
            "Epoch 1 Batch 4550 Loss 2.8682\n",
            "Epoch 1 Batch 4600 Loss 2.8570\n",
            "Epoch 1 Batch 4650 Loss 2.8460\n",
            "Epoch 1 Batch 4700 Loss 2.8345\n",
            "Epoch 1 Batch 4750 Loss 2.8235\n",
            "Epoch 1 Batch 4800 Loss 2.8130\n",
            "Epoch 1 Batch 4850 Loss 2.8023\n",
            "Epoch 1 Batch 4900 Loss 2.7915\n",
            "Epoch 1 Batch 4950 Loss 2.7809\n",
            "Epoch 1 Batch 5000 Loss 2.7707\n",
            "Epoch 1 Batch 5050 Loss 2.7605\n",
            "Epoch 1 Batch 5100 Loss 2.7501\n",
            "Epoch 1 Batch 5150 Loss 2.7401\n",
            "Epoch 1 Batch 5200 Loss 2.7299\n",
            "Epoch 1 Batch 5250 Loss 2.7201\n",
            "Epoch 1 Batch 5300 Loss 2.7103\n",
            "Epoch 1 Batch 5350 Loss 2.7008\n",
            "Epoch 1 Batch 5400 Loss 2.6922\n",
            "Epoch 1 Batch 5450 Loss 2.6842\n",
            "Epoch 1 Batch 5500 Loss 2.6766\n",
            "Epoch 1 Batch 5550 Loss 2.6698\n",
            "Epoch 1 Batch 5600 Loss 2.6627\n",
            "Epoch 1 Batch 5650 Loss 2.6558\n",
            "Epoch 1 Batch 5700 Loss 2.6490\n",
            "Epoch 1 Batch 5750 Loss 2.6424\n",
            "Epoch 1 Batch 5800 Loss 2.6358\n",
            "Epoch 1 Batch 5850 Loss 2.6292\n",
            "Epoch 1 Batch 5900 Loss 2.6226\n",
            "Epoch 1 Batch 5950 Loss 2.6162\n",
            "Epoch 1 Batch 6000 Loss 2.6099\n",
            "Epoch 1 Batch 6050 Loss 2.6037\n",
            "Epoch 1 Batch 6100 Loss 2.5973\n",
            "Epoch 1 Batch 6150 Loss 2.5911\n",
            "Epoch 1 Batch 6200 Loss 2.5851\n",
            "Epoch 1 Batch 6250 Loss 2.5790\n",
            "Epoch 1 Batch 6300 Loss 2.5730\n",
            "Epoch 1 Batch 6350 Loss 2.5670\n",
            "Epoch 1 Batch 6400 Loss 2.5610\n",
            "Epoch 1 Batch 6450 Loss 2.5552\n",
            "Epoch 1 Batch 6500 Loss 2.5493\n",
            "Epoch 1 Batch 6550 Loss 2.5434\n",
            "Epoch 1 Batch 6600 Loss 2.5375\n",
            "Epoch 1 Batch 6650 Loss 2.5315\n",
            "Epoch 1 Batch 6700 Loss 2.5255\n",
            "Epoch 1 Batch 6750 Loss 2.5197\n",
            "Epoch 1 Batch 6800 Loss 2.5139\n",
            "Epoch 1 Batch 6850 Loss 2.5083\n",
            "Epoch 1 Batch 6900 Loss 2.5027\n",
            "Epoch 1 Batch 6950 Loss 2.4968\n",
            "Saving checkpoint for epoch 1 at ./store/ckpt/10hepo/ckpt-1\n",
            "Time taken for 1 epoch: 1821.111979484558 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7096\n",
            "Epoch 2 Batch 50 Loss 1.7149\n",
            "Epoch 2 Batch 100 Loss 1.7009\n",
            "Epoch 2 Batch 150 Loss 1.6684\n",
            "Epoch 2 Batch 200 Loss 1.6549\n",
            "Epoch 2 Batch 250 Loss 1.6569\n",
            "Epoch 2 Batch 300 Loss 1.6582\n",
            "Epoch 2 Batch 350 Loss 1.6556\n",
            "Epoch 2 Batch 400 Loss 1.6509\n",
            "Epoch 2 Batch 450 Loss 1.6487\n",
            "Epoch 2 Batch 500 Loss 1.6444\n",
            "Epoch 2 Batch 550 Loss 1.6404\n",
            "Epoch 2 Batch 600 Loss 1.6320\n",
            "Epoch 2 Batch 650 Loss 1.6284\n",
            "Epoch 2 Batch 700 Loss 1.6210\n",
            "Epoch 2 Batch 750 Loss 1.6170\n",
            "Epoch 2 Batch 800 Loss 1.6132\n",
            "Epoch 2 Batch 850 Loss 1.6101\n",
            "Epoch 2 Batch 900 Loss 1.6091\n",
            "Epoch 2 Batch 950 Loss 1.6071\n",
            "Epoch 2 Batch 1000 Loss 1.6058\n",
            "Epoch 2 Batch 1050 Loss 1.6054\n",
            "Epoch 2 Batch 1100 Loss 1.6031\n",
            "Epoch 2 Batch 1150 Loss 1.6013\n",
            "Epoch 2 Batch 1200 Loss 1.5991\n",
            "Epoch 2 Batch 1250 Loss 1.5972\n",
            "Epoch 2 Batch 1300 Loss 1.5951\n",
            "Epoch 2 Batch 1350 Loss 1.5932\n",
            "Epoch 2 Batch 1400 Loss 1.5908\n",
            "Epoch 2 Batch 1450 Loss 1.5883\n",
            "Epoch 2 Batch 1500 Loss 1.5851\n",
            "Epoch 2 Batch 1550 Loss 1.5818\n",
            "Epoch 2 Batch 1600 Loss 1.5780\n",
            "Epoch 2 Batch 1650 Loss 1.5743\n",
            "Epoch 2 Batch 1700 Loss 1.5704\n",
            "Epoch 2 Batch 1750 Loss 1.5665\n",
            "Epoch 2 Batch 1800 Loss 1.5624\n",
            "Epoch 2 Batch 1850 Loss 1.5573\n",
            "Epoch 2 Batch 1900 Loss 1.5525\n",
            "Epoch 2 Batch 1950 Loss 1.5474\n",
            "Epoch 2 Batch 2000 Loss 1.5425\n",
            "Epoch 2 Batch 2050 Loss 1.5385\n",
            "Epoch 2 Batch 2100 Loss 1.5342\n",
            "Epoch 2 Batch 2150 Loss 1.5294\n",
            "Epoch 2 Batch 2200 Loss 1.5251\n",
            "Epoch 2 Batch 2250 Loss 1.5202\n",
            "Epoch 2 Batch 2300 Loss 1.5147\n",
            "Epoch 2 Batch 2350 Loss 1.5100\n",
            "Epoch 2 Batch 2400 Loss 1.5051\n",
            "Epoch 2 Batch 2450 Loss 1.5005\n",
            "Epoch 2 Batch 2500 Loss 1.4971\n",
            "Epoch 2 Batch 2550 Loss 1.4930\n",
            "Epoch 2 Batch 2600 Loss 1.4888\n",
            "Epoch 2 Batch 2650 Loss 1.4848\n",
            "Epoch 2 Batch 2700 Loss 1.4801\n",
            "Epoch 2 Batch 2750 Loss 1.4757\n",
            "Epoch 2 Batch 2800 Loss 1.4715\n",
            "Epoch 2 Batch 2850 Loss 1.4669\n",
            "Epoch 2 Batch 2900 Loss 1.4628\n",
            "Epoch 2 Batch 2950 Loss 1.4587\n",
            "Epoch 2 Batch 3000 Loss 1.4549\n",
            "Epoch 2 Batch 3050 Loss 1.4511\n",
            "Epoch 2 Batch 3100 Loss 1.4473\n",
            "Epoch 2 Batch 3150 Loss 1.4446\n",
            "Epoch 2 Batch 3200 Loss 1.4410\n",
            "Epoch 2 Batch 3250 Loss 1.4378\n",
            "Epoch 2 Batch 3300 Loss 1.4350\n",
            "Epoch 2 Batch 3350 Loss 1.4321\n",
            "Epoch 2 Batch 3400 Loss 1.4291\n",
            "Epoch 2 Batch 3450 Loss 1.4263\n",
            "Epoch 2 Batch 3500 Loss 1.4233\n",
            "Epoch 2 Batch 3550 Loss 1.4208\n",
            "Epoch 2 Batch 3600 Loss 1.4184\n",
            "Epoch 2 Batch 3650 Loss 1.4157\n",
            "Epoch 2 Batch 3700 Loss 1.4133\n",
            "Epoch 2 Batch 3750 Loss 1.4112\n",
            "Epoch 2 Batch 3800 Loss 1.4089\n",
            "Epoch 2 Batch 3850 Loss 1.4067\n",
            "Epoch 2 Batch 3900 Loss 1.4046\n",
            "Epoch 2 Batch 3950 Loss 1.4027\n",
            "Epoch 2 Batch 4000 Loss 1.4006\n",
            "Epoch 2 Batch 4050 Loss 1.3984\n",
            "Epoch 2 Batch 4100 Loss 1.3961\n",
            "Epoch 2 Batch 4150 Loss 1.3937\n",
            "Epoch 2 Batch 4200 Loss 1.3915\n",
            "Epoch 2 Batch 4250 Loss 1.3892\n",
            "Epoch 2 Batch 4300 Loss 1.3875\n",
            "Epoch 2 Batch 4350 Loss 1.3853\n",
            "Epoch 2 Batch 4400 Loss 1.3829\n",
            "Epoch 2 Batch 4450 Loss 1.3812\n",
            "Epoch 2 Batch 4500 Loss 1.3789\n",
            "Epoch 2 Batch 4550 Loss 1.3769\n",
            "Epoch 2 Batch 4600 Loss 1.3750\n",
            "Epoch 2 Batch 4650 Loss 1.3730\n",
            "Epoch 2 Batch 4700 Loss 1.3713\n",
            "Epoch 2 Batch 4750 Loss 1.3696\n",
            "Epoch 2 Batch 4800 Loss 1.3678\n",
            "Epoch 2 Batch 4850 Loss 1.3659\n",
            "Epoch 2 Batch 4900 Loss 1.3640\n",
            "Epoch 2 Batch 4950 Loss 1.3621\n",
            "Epoch 2 Batch 5000 Loss 1.3605\n",
            "Epoch 2 Batch 5050 Loss 1.3589\n",
            "Epoch 2 Batch 5100 Loss 1.3573\n",
            "Epoch 2 Batch 5150 Loss 1.3559\n",
            "Epoch 2 Batch 5200 Loss 1.3542\n",
            "Epoch 2 Batch 5250 Loss 1.3522\n",
            "Epoch 2 Batch 5300 Loss 1.3505\n",
            "Epoch 2 Batch 5350 Loss 1.3496\n",
            "Epoch 2 Batch 5400 Loss 1.3490\n",
            "Epoch 2 Batch 5450 Loss 1.3489\n",
            "Epoch 2 Batch 5500 Loss 1.3492\n",
            "Epoch 2 Batch 5550 Loss 1.3499\n",
            "Epoch 2 Batch 5600 Loss 1.3509\n",
            "Epoch 2 Batch 5650 Loss 1.3519\n",
            "Epoch 2 Batch 5700 Loss 1.3531\n",
            "Epoch 2 Batch 5750 Loss 1.3541\n",
            "Epoch 2 Batch 5800 Loss 1.3548\n",
            "Epoch 2 Batch 5850 Loss 1.3556\n",
            "Epoch 2 Batch 5900 Loss 1.3567\n",
            "Epoch 2 Batch 5950 Loss 1.3577\n",
            "Epoch 2 Batch 6000 Loss 1.3583\n",
            "Epoch 2 Batch 6050 Loss 1.3587\n",
            "Epoch 2 Batch 6100 Loss 1.3594\n",
            "Epoch 2 Batch 6150 Loss 1.3604\n",
            "Epoch 2 Batch 6200 Loss 1.3613\n",
            "Epoch 2 Batch 6250 Loss 1.3622\n",
            "Epoch 2 Batch 6300 Loss 1.3633\n",
            "Epoch 2 Batch 6350 Loss 1.3643\n",
            "Epoch 2 Batch 6400 Loss 1.3650\n",
            "Epoch 2 Batch 6450 Loss 1.3657\n",
            "Epoch 2 Batch 6500 Loss 1.3664\n",
            "Epoch 2 Batch 6550 Loss 1.3671\n",
            "Epoch 2 Batch 6600 Loss 1.3675\n",
            "Epoch 2 Batch 6650 Loss 1.3681\n",
            "Epoch 2 Batch 6700 Loss 1.3685\n",
            "Epoch 2 Batch 6750 Loss 1.3689\n",
            "Epoch 2 Batch 6800 Loss 1.3694\n",
            "Epoch 2 Batch 6850 Loss 1.3697\n",
            "Epoch 2 Batch 6900 Loss 1.3697\n",
            "Epoch 2 Batch 6950 Loss 1.3698\n",
            "Saving checkpoint for epoch 2 at ./store/ckpt/10hepo/ckpt-2\n",
            "Time taken for 1 epoch: 1742.8187618255615 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.3362\n",
            "Epoch 3 Batch 50 Loss 1.3920\n",
            "Epoch 3 Batch 100 Loss 1.3845\n",
            "Epoch 3 Batch 150 Loss 1.3798\n",
            "Epoch 3 Batch 200 Loss 1.3712\n",
            "Epoch 3 Batch 250 Loss 1.3663\n",
            "Epoch 3 Batch 300 Loss 1.3626\n",
            "Epoch 3 Batch 350 Loss 1.3603\n",
            "Epoch 3 Batch 400 Loss 1.3540\n",
            "Epoch 3 Batch 450 Loss 1.3512\n",
            "Epoch 3 Batch 500 Loss 1.3508\n",
            "Epoch 3 Batch 550 Loss 1.3511\n",
            "Epoch 3 Batch 600 Loss 1.3478\n",
            "Epoch 3 Batch 650 Loss 1.3457\n",
            "Epoch 3 Batch 700 Loss 1.3420\n",
            "Epoch 3 Batch 750 Loss 1.3389\n",
            "Epoch 3 Batch 800 Loss 1.3375\n",
            "Epoch 3 Batch 850 Loss 1.3360\n",
            "Epoch 3 Batch 900 Loss 1.3362\n",
            "Epoch 3 Batch 950 Loss 1.3350\n",
            "Epoch 3 Batch 1000 Loss 1.3345\n",
            "Epoch 3 Batch 1050 Loss 1.3340\n",
            "Epoch 3 Batch 1100 Loss 1.3332\n",
            "Epoch 3 Batch 1150 Loss 1.3334\n",
            "Epoch 3 Batch 1200 Loss 1.3333\n",
            "Epoch 3 Batch 1250 Loss 1.3327\n",
            "Epoch 3 Batch 1300 Loss 1.3309\n",
            "Epoch 3 Batch 1350 Loss 1.3297\n",
            "Epoch 3 Batch 1400 Loss 1.3286\n",
            "Epoch 3 Batch 1450 Loss 1.3268\n",
            "Epoch 3 Batch 1500 Loss 1.3240\n",
            "Epoch 3 Batch 1550 Loss 1.3217\n",
            "Epoch 3 Batch 1600 Loss 1.3190\n",
            "Epoch 3 Batch 1650 Loss 1.3151\n",
            "Epoch 3 Batch 1700 Loss 1.3110\n",
            "Epoch 3 Batch 1750 Loss 1.3086\n",
            "Epoch 3 Batch 1800 Loss 1.3053\n",
            "Epoch 3 Batch 1850 Loss 1.3019\n",
            "Epoch 3 Batch 1900 Loss 1.2980\n",
            "Epoch 3 Batch 1950 Loss 1.2941\n",
            "Epoch 3 Batch 2000 Loss 1.2898\n",
            "Epoch 3 Batch 2050 Loss 1.2866\n",
            "Epoch 3 Batch 2100 Loss 1.2825\n",
            "Epoch 3 Batch 2150 Loss 1.2799\n",
            "Epoch 3 Batch 2200 Loss 1.2763\n",
            "Epoch 3 Batch 2250 Loss 1.2731\n",
            "Epoch 3 Batch 2300 Loss 1.2697\n",
            "Epoch 3 Batch 2350 Loss 1.2659\n",
            "Epoch 3 Batch 2400 Loss 1.2619\n",
            "Epoch 3 Batch 2450 Loss 1.2584\n",
            "Epoch 3 Batch 2500 Loss 1.2554\n",
            "Epoch 3 Batch 2550 Loss 1.2522\n",
            "Epoch 3 Batch 2600 Loss 1.2491\n",
            "Epoch 3 Batch 2650 Loss 1.2455\n",
            "Epoch 3 Batch 2700 Loss 1.2422\n",
            "Epoch 3 Batch 2750 Loss 1.2381\n",
            "Epoch 3 Batch 2800 Loss 1.2347\n",
            "Epoch 3 Batch 2850 Loss 1.2314\n",
            "Epoch 3 Batch 2900 Loss 1.2284\n",
            "Epoch 3 Batch 2950 Loss 1.2250\n",
            "Epoch 3 Batch 3000 Loss 1.2221\n",
            "Epoch 3 Batch 3050 Loss 1.2191\n",
            "Epoch 3 Batch 3100 Loss 1.2168\n",
            "Epoch 3 Batch 3150 Loss 1.2138\n",
            "Epoch 3 Batch 3200 Loss 1.2111\n",
            "Epoch 3 Batch 3250 Loss 1.2085\n",
            "Epoch 3 Batch 3300 Loss 1.2063\n",
            "Epoch 3 Batch 3350 Loss 1.2043\n",
            "Epoch 3 Batch 3400 Loss 1.2021\n",
            "Epoch 3 Batch 3450 Loss 1.2002\n",
            "Epoch 3 Batch 3500 Loss 1.1982\n",
            "Epoch 3 Batch 3550 Loss 1.1967\n",
            "Epoch 3 Batch 3600 Loss 1.1950\n",
            "Epoch 3 Batch 3650 Loss 1.1934\n",
            "Epoch 3 Batch 3700 Loss 1.1918\n",
            "Epoch 3 Batch 3750 Loss 1.1902\n",
            "Epoch 3 Batch 3800 Loss 1.1890\n",
            "Epoch 3 Batch 3850 Loss 1.1875\n",
            "Epoch 3 Batch 3900 Loss 1.1860\n",
            "Epoch 3 Batch 3950 Loss 1.1846\n",
            "Epoch 3 Batch 4000 Loss 1.1829\n",
            "Epoch 3 Batch 4050 Loss 1.1816\n",
            "Epoch 3 Batch 4100 Loss 1.1801\n",
            "Epoch 3 Batch 4150 Loss 1.1790\n",
            "Epoch 3 Batch 4200 Loss 1.1776\n",
            "Epoch 3 Batch 4250 Loss 1.1765\n",
            "Epoch 3 Batch 4300 Loss 1.1753\n",
            "Epoch 3 Batch 4350 Loss 1.1737\n",
            "Epoch 3 Batch 4400 Loss 1.1725\n",
            "Epoch 3 Batch 4450 Loss 1.1713\n",
            "Epoch 3 Batch 4500 Loss 1.1700\n",
            "Epoch 3 Batch 4550 Loss 1.1691\n",
            "Epoch 3 Batch 4600 Loss 1.1679\n",
            "Epoch 3 Batch 4650 Loss 1.1666\n",
            "Epoch 3 Batch 4700 Loss 1.1652\n",
            "Epoch 3 Batch 4750 Loss 1.1643\n",
            "Epoch 3 Batch 4800 Loss 1.1627\n",
            "Epoch 3 Batch 4850 Loss 1.1615\n",
            "Epoch 3 Batch 4900 Loss 1.1604\n",
            "Epoch 3 Batch 4950 Loss 1.1594\n",
            "Epoch 3 Batch 5000 Loss 1.1583\n",
            "Epoch 3 Batch 5050 Loss 1.1571\n",
            "Epoch 3 Batch 5100 Loss 1.1561\n",
            "Epoch 3 Batch 5150 Loss 1.1551\n",
            "Epoch 3 Batch 5200 Loss 1.1544\n",
            "Epoch 3 Batch 5250 Loss 1.1534\n",
            "Epoch 3 Batch 5300 Loss 1.1525\n",
            "Epoch 3 Batch 5350 Loss 1.1520\n",
            "Epoch 3 Batch 5400 Loss 1.1524\n",
            "Epoch 3 Batch 5450 Loss 1.1534\n",
            "Epoch 3 Batch 5500 Loss 1.1542\n",
            "Epoch 3 Batch 5550 Loss 1.1555\n",
            "Epoch 3 Batch 5600 Loss 1.1566\n",
            "Epoch 3 Batch 5650 Loss 1.1581\n",
            "Epoch 3 Batch 5700 Loss 1.1597\n",
            "Epoch 3 Batch 5750 Loss 1.1611\n",
            "Epoch 3 Batch 5800 Loss 1.1626\n",
            "Epoch 3 Batch 5850 Loss 1.1643\n",
            "Epoch 3 Batch 5900 Loss 1.1660\n",
            "Epoch 3 Batch 5950 Loss 1.1670\n",
            "Epoch 3 Batch 6000 Loss 1.1683\n",
            "Epoch 3 Batch 6050 Loss 1.1697\n",
            "Epoch 3 Batch 6100 Loss 1.1710\n",
            "Epoch 3 Batch 6150 Loss 1.1724\n",
            "Epoch 3 Batch 6200 Loss 1.1737\n",
            "Epoch 3 Batch 6250 Loss 1.1751\n",
            "Epoch 3 Batch 6300 Loss 1.1763\n",
            "Epoch 3 Batch 6350 Loss 1.1777\n",
            "Epoch 3 Batch 6400 Loss 1.1790\n",
            "Epoch 3 Batch 6450 Loss 1.1802\n",
            "Epoch 3 Batch 6500 Loss 1.1814\n",
            "Epoch 3 Batch 6550 Loss 1.1827\n",
            "Epoch 3 Batch 6600 Loss 1.1838\n",
            "Epoch 3 Batch 6650 Loss 1.1850\n",
            "Epoch 3 Batch 6700 Loss 1.1860\n",
            "Epoch 3 Batch 6750 Loss 1.1870\n",
            "Epoch 3 Batch 6800 Loss 1.1879\n",
            "Epoch 3 Batch 6850 Loss 1.1890\n",
            "Epoch 3 Batch 6900 Loss 1.1899\n",
            "Epoch 3 Batch 6950 Loss 1.1908\n",
            "Saving checkpoint for epoch 3 at ./store/ckpt/10hepo/ckpt-3\n",
            "Time taken for 1 epoch: 1744.8621060848236 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.4459\n",
            "Epoch 4 Batch 50 Loss 1.2792\n",
            "Epoch 4 Batch 100 Loss 1.2652\n",
            "Epoch 4 Batch 150 Loss 1.2626\n",
            "Epoch 4 Batch 200 Loss 1.2711\n",
            "Epoch 4 Batch 250 Loss 1.2636\n",
            "Epoch 4 Batch 300 Loss 1.2627\n",
            "Epoch 4 Batch 350 Loss 1.2596\n",
            "Epoch 4 Batch 400 Loss 1.2558\n",
            "Epoch 4 Batch 450 Loss 1.2546\n",
            "Epoch 4 Batch 500 Loss 1.2539\n",
            "Epoch 4 Batch 550 Loss 1.2494\n",
            "Epoch 4 Batch 600 Loss 1.2475\n",
            "Epoch 4 Batch 650 Loss 1.2433\n",
            "Epoch 4 Batch 700 Loss 1.2426\n",
            "Epoch 4 Batch 750 Loss 1.2397\n",
            "Epoch 4 Batch 800 Loss 1.2365\n",
            "Epoch 4 Batch 850 Loss 1.2340\n",
            "Epoch 4 Batch 900 Loss 1.2335\n",
            "Epoch 4 Batch 950 Loss 1.2326\n",
            "Epoch 4 Batch 1000 Loss 1.2331\n",
            "Epoch 4 Batch 1050 Loss 1.2339\n",
            "Epoch 4 Batch 1100 Loss 1.2343\n",
            "Epoch 4 Batch 1150 Loss 1.2347\n",
            "Epoch 4 Batch 1200 Loss 1.2337\n",
            "Epoch 4 Batch 1250 Loss 1.2333\n",
            "Epoch 4 Batch 1300 Loss 1.2326\n",
            "Epoch 4 Batch 1350 Loss 1.2315\n",
            "Epoch 4 Batch 1400 Loss 1.2289\n",
            "Epoch 4 Batch 1450 Loss 1.2278\n",
            "Epoch 4 Batch 1500 Loss 1.2252\n",
            "Epoch 4 Batch 1550 Loss 1.2219\n",
            "Epoch 4 Batch 1600 Loss 1.2189\n",
            "Epoch 4 Batch 1650 Loss 1.2161\n",
            "Epoch 4 Batch 1700 Loss 1.2143\n",
            "Epoch 4 Batch 1750 Loss 1.2108\n",
            "Epoch 4 Batch 1800 Loss 1.2071\n",
            "Epoch 4 Batch 1850 Loss 1.2035\n",
            "Epoch 4 Batch 1900 Loss 1.1991\n",
            "Epoch 4 Batch 1950 Loss 1.1967\n",
            "Epoch 4 Batch 2000 Loss 1.1932\n",
            "Epoch 4 Batch 2050 Loss 1.1898\n",
            "Epoch 4 Batch 2100 Loss 1.1867\n",
            "Epoch 4 Batch 2150 Loss 1.1837\n",
            "Epoch 4 Batch 2200 Loss 1.1802\n",
            "Epoch 4 Batch 2250 Loss 1.1763\n",
            "Epoch 4 Batch 2300 Loss 1.1731\n",
            "Epoch 4 Batch 2350 Loss 1.1696\n",
            "Epoch 4 Batch 2400 Loss 1.1665\n",
            "Epoch 4 Batch 2450 Loss 1.1632\n",
            "Epoch 4 Batch 2500 Loss 1.1604\n",
            "Epoch 4 Batch 2550 Loss 1.1572\n",
            "Epoch 4 Batch 2600 Loss 1.1541\n",
            "Epoch 4 Batch 2650 Loss 1.1507\n",
            "Epoch 4 Batch 2700 Loss 1.1467\n",
            "Epoch 4 Batch 2750 Loss 1.1438\n",
            "Epoch 4 Batch 2800 Loss 1.1407\n",
            "Epoch 4 Batch 2850 Loss 1.1373\n",
            "Epoch 4 Batch 2900 Loss 1.1336\n",
            "Epoch 4 Batch 2950 Loss 1.1308\n",
            "Epoch 4 Batch 3000 Loss 1.1283\n",
            "Epoch 4 Batch 3050 Loss 1.1259\n",
            "Epoch 4 Batch 3100 Loss 1.1233\n",
            "Epoch 4 Batch 3150 Loss 1.1207\n",
            "Epoch 4 Batch 3200 Loss 1.1182\n",
            "Epoch 4 Batch 3250 Loss 1.1162\n",
            "Epoch 4 Batch 3300 Loss 1.1139\n",
            "Epoch 4 Batch 3350 Loss 1.1117\n",
            "Epoch 4 Batch 3400 Loss 1.1094\n",
            "Epoch 4 Batch 3450 Loss 1.1082\n",
            "Epoch 4 Batch 3500 Loss 1.1066\n",
            "Epoch 4 Batch 3550 Loss 1.1050\n",
            "Epoch 4 Batch 3600 Loss 1.1035\n",
            "Epoch 4 Batch 3650 Loss 1.1020\n",
            "Epoch 4 Batch 3700 Loss 1.1008\n",
            "Epoch 4 Batch 3750 Loss 1.0992\n",
            "Epoch 4 Batch 3800 Loss 1.0982\n",
            "Epoch 4 Batch 3850 Loss 1.0971\n",
            "Epoch 4 Batch 3900 Loss 1.0960\n",
            "Epoch 4 Batch 3950 Loss 1.0947\n",
            "Epoch 4 Batch 4000 Loss 1.0935\n",
            "Epoch 4 Batch 4050 Loss 1.0921\n",
            "Epoch 4 Batch 4100 Loss 1.0907\n",
            "Epoch 4 Batch 4150 Loss 1.0897\n",
            "Epoch 4 Batch 4200 Loss 1.0885\n",
            "Epoch 4 Batch 4250 Loss 1.0872\n",
            "Epoch 4 Batch 4300 Loss 1.0863\n",
            "Epoch 4 Batch 4350 Loss 1.0850\n",
            "Epoch 4 Batch 4400 Loss 1.0841\n",
            "Epoch 4 Batch 4450 Loss 1.0830\n",
            "Epoch 4 Batch 4500 Loss 1.0822\n",
            "Epoch 4 Batch 4550 Loss 1.0811\n",
            "Epoch 4 Batch 4600 Loss 1.0804\n",
            "Epoch 4 Batch 4650 Loss 1.0796\n",
            "Epoch 4 Batch 4700 Loss 1.0782\n",
            "Epoch 4 Batch 4750 Loss 1.0775\n",
            "Epoch 4 Batch 4800 Loss 1.0764\n",
            "Epoch 4 Batch 4850 Loss 1.0754\n",
            "Epoch 4 Batch 4900 Loss 1.0744\n",
            "Epoch 4 Batch 4950 Loss 1.0737\n",
            "Epoch 4 Batch 5000 Loss 1.0730\n",
            "Epoch 4 Batch 5050 Loss 1.0721\n",
            "Epoch 4 Batch 5100 Loss 1.0712\n",
            "Epoch 4 Batch 5150 Loss 1.0705\n",
            "Epoch 4 Batch 5200 Loss 1.0696\n",
            "Epoch 4 Batch 5250 Loss 1.0689\n",
            "Epoch 4 Batch 5300 Loss 1.0680\n",
            "Epoch 4 Batch 5350 Loss 1.0675\n",
            "Epoch 4 Batch 5400 Loss 1.0680\n",
            "Epoch 4 Batch 5450 Loss 1.0688\n",
            "Epoch 4 Batch 5500 Loss 1.0699\n",
            "Epoch 4 Batch 5550 Loss 1.0712\n",
            "Epoch 4 Batch 5600 Loss 1.0729\n",
            "Epoch 4 Batch 5650 Loss 1.0744\n",
            "Epoch 4 Batch 5700 Loss 1.0761\n",
            "Epoch 4 Batch 5750 Loss 1.0778\n",
            "Epoch 4 Batch 5800 Loss 1.0797\n",
            "Epoch 4 Batch 5850 Loss 1.0812\n",
            "Epoch 4 Batch 5900 Loss 1.0827\n",
            "Epoch 4 Batch 5950 Loss 1.0844\n",
            "Epoch 4 Batch 6000 Loss 1.0856\n",
            "Epoch 4 Batch 6050 Loss 1.0868\n",
            "Epoch 4 Batch 6100 Loss 1.0884\n",
            "Epoch 4 Batch 6150 Loss 1.0902\n",
            "Epoch 4 Batch 6200 Loss 1.0917\n",
            "Epoch 4 Batch 6250 Loss 1.0933\n",
            "Epoch 4 Batch 6300 Loss 1.0948\n",
            "Epoch 4 Batch 6350 Loss 1.0963\n",
            "Epoch 4 Batch 6400 Loss 1.0977\n",
            "Epoch 4 Batch 6450 Loss 1.0993\n",
            "Epoch 4 Batch 6500 Loss 1.1009\n",
            "Epoch 4 Batch 6550 Loss 1.1023\n",
            "Epoch 4 Batch 6600 Loss 1.1035\n",
            "Epoch 4 Batch 6650 Loss 1.1046\n",
            "Epoch 4 Batch 6700 Loss 1.1059\n",
            "Epoch 4 Batch 6750 Loss 1.1070\n",
            "Epoch 4 Batch 6800 Loss 1.1080\n",
            "Epoch 4 Batch 6850 Loss 1.1090\n",
            "Epoch 4 Batch 6900 Loss 1.1101\n",
            "Epoch 4 Batch 6950 Loss 1.1112\n",
            "Saving checkpoint for epoch 4 at ./store/ckpt/10hepo/ckpt-4\n",
            "Time taken for 1 epoch: 1754.6897897720337 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.2289\n",
            "Epoch 5 Batch 50 Loss 1.2190\n",
            "Epoch 5 Batch 100 Loss 1.1996\n",
            "Epoch 5 Batch 150 Loss 1.2010\n",
            "Epoch 5 Batch 200 Loss 1.2036\n",
            "Epoch 5 Batch 250 Loss 1.2054\n",
            "Epoch 5 Batch 300 Loss 1.2038\n",
            "Epoch 5 Batch 350 Loss 1.2006\n",
            "Epoch 5 Batch 400 Loss 1.1954\n",
            "Epoch 5 Batch 450 Loss 1.1915\n",
            "Epoch 5 Batch 500 Loss 1.1895\n",
            "Epoch 5 Batch 550 Loss 1.1871\n",
            "Epoch 5 Batch 600 Loss 1.1839\n",
            "Epoch 5 Batch 650 Loss 1.1810\n",
            "Epoch 5 Batch 700 Loss 1.1805\n",
            "Epoch 5 Batch 750 Loss 1.1772\n",
            "Epoch 5 Batch 800 Loss 1.1767\n",
            "Epoch 5 Batch 850 Loss 1.1766\n",
            "Epoch 5 Batch 900 Loss 1.1757\n",
            "Epoch 5 Batch 950 Loss 1.1748\n",
            "Epoch 5 Batch 1000 Loss 1.1741\n",
            "Epoch 5 Batch 1050 Loss 1.1727\n",
            "Epoch 5 Batch 1100 Loss 1.1738\n",
            "Epoch 5 Batch 1150 Loss 1.1747\n",
            "Epoch 5 Batch 1200 Loss 1.1768\n",
            "Epoch 5 Batch 1250 Loss 1.1754\n",
            "Epoch 5 Batch 1300 Loss 1.1751\n",
            "Epoch 5 Batch 1350 Loss 1.1738\n",
            "Epoch 5 Batch 1400 Loss 1.1718\n",
            "Epoch 5 Batch 1450 Loss 1.1703\n",
            "Epoch 5 Batch 1500 Loss 1.1677\n",
            "Epoch 5 Batch 1550 Loss 1.1654\n",
            "Epoch 5 Batch 1600 Loss 1.1616\n",
            "Epoch 5 Batch 1650 Loss 1.1590\n",
            "Epoch 5 Batch 1700 Loss 1.1557\n",
            "Epoch 5 Batch 1750 Loss 1.1519\n",
            "Epoch 5 Batch 1800 Loss 1.1492\n",
            "Epoch 5 Batch 1850 Loss 1.1464\n",
            "Epoch 5 Batch 1900 Loss 1.1430\n",
            "Epoch 5 Batch 1950 Loss 1.1400\n",
            "Epoch 5 Batch 2000 Loss 1.1369\n",
            "Epoch 5 Batch 2050 Loss 1.1333\n",
            "Epoch 5 Batch 2100 Loss 1.1299\n",
            "Epoch 5 Batch 2150 Loss 1.1270\n",
            "Epoch 5 Batch 2200 Loss 1.1237\n",
            "Epoch 5 Batch 2250 Loss 1.1198\n",
            "Epoch 5 Batch 2300 Loss 1.1170\n",
            "Epoch 5 Batch 2350 Loss 1.1134\n",
            "Epoch 5 Batch 2400 Loss 1.1096\n",
            "Epoch 5 Batch 2450 Loss 1.1064\n",
            "Epoch 5 Batch 2500 Loss 1.1033\n",
            "Epoch 5 Batch 2550 Loss 1.1007\n",
            "Epoch 5 Batch 2600 Loss 1.0976\n",
            "Epoch 5 Batch 2650 Loss 1.0944\n",
            "Epoch 5 Batch 2700 Loss 1.0912\n",
            "Epoch 5 Batch 2750 Loss 1.0881\n",
            "Epoch 5 Batch 2800 Loss 1.0847\n",
            "Epoch 5 Batch 2850 Loss 1.0815\n",
            "Epoch 5 Batch 2900 Loss 1.0787\n",
            "Epoch 5 Batch 2950 Loss 1.0756\n",
            "Epoch 5 Batch 3000 Loss 1.0726\n",
            "Epoch 5 Batch 3050 Loss 1.0700\n",
            "Epoch 5 Batch 3100 Loss 1.0678\n",
            "Epoch 5 Batch 3150 Loss 1.0655\n",
            "Epoch 5 Batch 3200 Loss 1.0632\n",
            "Epoch 5 Batch 3250 Loss 1.0612\n",
            "Epoch 5 Batch 3300 Loss 1.0592\n",
            "Epoch 5 Batch 3350 Loss 1.0575\n",
            "Epoch 5 Batch 3400 Loss 1.0556\n",
            "Epoch 5 Batch 3450 Loss 1.0536\n",
            "Epoch 5 Batch 3500 Loss 1.0520\n",
            "Epoch 5 Batch 3550 Loss 1.0504\n",
            "Epoch 5 Batch 3600 Loss 1.0496\n",
            "Epoch 5 Batch 3650 Loss 1.0482\n",
            "Epoch 5 Batch 3700 Loss 1.0469\n",
            "Epoch 5 Batch 3750 Loss 1.0455\n",
            "Epoch 5 Batch 3800 Loss 1.0442\n",
            "Epoch 5 Batch 3850 Loss 1.0429\n",
            "Epoch 5 Batch 3900 Loss 1.0420\n",
            "Epoch 5 Batch 3950 Loss 1.0412\n",
            "Epoch 5 Batch 4000 Loss 1.0399\n",
            "Epoch 5 Batch 4050 Loss 1.0393\n",
            "Epoch 5 Batch 4100 Loss 1.0379\n",
            "Epoch 5 Batch 4150 Loss 1.0370\n",
            "Epoch 5 Batch 4200 Loss 1.0358\n",
            "Epoch 5 Batch 4250 Loss 1.0347\n",
            "Epoch 5 Batch 4300 Loss 1.0337\n",
            "Epoch 5 Batch 4350 Loss 1.0328\n",
            "Epoch 5 Batch 4400 Loss 1.0319\n",
            "Epoch 5 Batch 4450 Loss 1.0309\n",
            "Epoch 5 Batch 4500 Loss 1.0299\n",
            "Epoch 5 Batch 4550 Loss 1.0288\n",
            "Epoch 5 Batch 4600 Loss 1.0278\n",
            "Epoch 5 Batch 4650 Loss 1.0267\n",
            "Epoch 5 Batch 4700 Loss 1.0257\n",
            "Epoch 5 Batch 4750 Loss 1.0249\n",
            "Epoch 5 Batch 4800 Loss 1.0238\n",
            "Epoch 5 Batch 4850 Loss 1.0229\n",
            "Epoch 5 Batch 4900 Loss 1.0222\n",
            "Epoch 5 Batch 4950 Loss 1.0214\n",
            "Epoch 5 Batch 5000 Loss 1.0209\n",
            "Epoch 5 Batch 5050 Loss 1.0203\n",
            "Epoch 5 Batch 5100 Loss 1.0193\n",
            "Epoch 5 Batch 5150 Loss 1.0187\n",
            "Epoch 5 Batch 5200 Loss 1.0179\n",
            "Epoch 5 Batch 5250 Loss 1.0170\n",
            "Epoch 5 Batch 5300 Loss 1.0162\n",
            "Epoch 5 Batch 5350 Loss 1.0162\n",
            "Epoch 5 Batch 5400 Loss 1.0169\n",
            "Epoch 5 Batch 5450 Loss 1.0179\n",
            "Epoch 5 Batch 5500 Loss 1.0191\n",
            "Epoch 5 Batch 5550 Loss 1.0205\n",
            "Epoch 5 Batch 5600 Loss 1.0220\n",
            "Epoch 5 Batch 5650 Loss 1.0237\n",
            "Epoch 5 Batch 5700 Loss 1.0256\n",
            "Epoch 5 Batch 5750 Loss 1.0274\n",
            "Epoch 5 Batch 5800 Loss 1.0293\n",
            "Epoch 5 Batch 5850 Loss 1.0310\n",
            "Epoch 5 Batch 5900 Loss 1.0327\n",
            "Epoch 5 Batch 5950 Loss 1.0340\n",
            "Epoch 5 Batch 6000 Loss 1.0355\n",
            "Epoch 5 Batch 6050 Loss 1.0370\n",
            "Epoch 5 Batch 6100 Loss 1.0386\n",
            "Epoch 5 Batch 6150 Loss 1.0400\n",
            "Epoch 5 Batch 6200 Loss 1.0414\n",
            "Epoch 5 Batch 6250 Loss 1.0431\n",
            "Epoch 5 Batch 6300 Loss 1.0446\n",
            "Epoch 5 Batch 6350 Loss 1.0466\n",
            "Epoch 5 Batch 6400 Loss 1.0480\n",
            "Epoch 5 Batch 6450 Loss 1.0495\n",
            "Epoch 5 Batch 6500 Loss 1.0508\n",
            "Epoch 5 Batch 6550 Loss 1.0523\n",
            "Epoch 5 Batch 6600 Loss 1.0537\n",
            "Epoch 5 Batch 6650 Loss 1.0548\n",
            "Epoch 5 Batch 6700 Loss 1.0562\n",
            "Epoch 5 Batch 6750 Loss 1.0575\n",
            "Epoch 5 Batch 6800 Loss 1.0586\n",
            "Epoch 5 Batch 6850 Loss 1.0598\n",
            "Epoch 5 Batch 6900 Loss 1.0607\n",
            "Epoch 5 Batch 6950 Loss 1.0620\n",
            "Saving checkpoint for epoch 5 at ./store/ckpt/10hepo/ckpt-5\n",
            "Time taken for 1 epoch: 1714.51242685318 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.2992\n",
            "Epoch 6 Batch 50 Loss 1.1867\n",
            "Epoch 6 Batch 100 Loss 1.1612\n",
            "Epoch 6 Batch 150 Loss 1.1721\n",
            "Epoch 6 Batch 200 Loss 1.1657\n",
            "Epoch 6 Batch 250 Loss 1.1639\n",
            "Epoch 6 Batch 300 Loss 1.1608\n",
            "Epoch 6 Batch 350 Loss 1.1555\n",
            "Epoch 6 Batch 400 Loss 1.1553\n",
            "Epoch 6 Batch 450 Loss 1.1549\n",
            "Epoch 6 Batch 500 Loss 1.1523\n",
            "Epoch 6 Batch 550 Loss 1.1488\n",
            "Epoch 6 Batch 600 Loss 1.1477\n",
            "Epoch 6 Batch 650 Loss 1.1444\n",
            "Epoch 6 Batch 700 Loss 1.1417\n",
            "Epoch 6 Batch 750 Loss 1.1396\n",
            "Epoch 6 Batch 800 Loss 1.1383\n",
            "Epoch 6 Batch 850 Loss 1.1357\n",
            "Epoch 6 Batch 900 Loss 1.1343\n",
            "Epoch 6 Batch 950 Loss 1.1359\n",
            "Epoch 6 Batch 1000 Loss 1.1362\n",
            "Epoch 6 Batch 1050 Loss 1.1370\n",
            "Epoch 6 Batch 1100 Loss 1.1370\n",
            "Epoch 6 Batch 1150 Loss 1.1376\n",
            "Epoch 6 Batch 1200 Loss 1.1381\n",
            "Epoch 6 Batch 1250 Loss 1.1377\n",
            "Epoch 6 Batch 1300 Loss 1.1367\n",
            "Epoch 6 Batch 1350 Loss 1.1357\n",
            "Epoch 6 Batch 1400 Loss 1.1349\n",
            "Epoch 6 Batch 1450 Loss 1.1330\n",
            "Epoch 6 Batch 1500 Loss 1.1300\n",
            "Epoch 6 Batch 1550 Loss 1.1276\n",
            "Epoch 6 Batch 1600 Loss 1.1247\n",
            "Epoch 6 Batch 1650 Loss 1.1214\n",
            "Epoch 6 Batch 1700 Loss 1.1179\n",
            "Epoch 6 Batch 1750 Loss 1.1147\n",
            "Epoch 6 Batch 1800 Loss 1.1107\n",
            "Epoch 6 Batch 1850 Loss 1.1073\n",
            "Epoch 6 Batch 1900 Loss 1.1039\n",
            "Epoch 6 Batch 1950 Loss 1.1008\n",
            "Epoch 6 Batch 2000 Loss 1.0982\n",
            "Epoch 6 Batch 2050 Loss 1.0951\n",
            "Epoch 6 Batch 2100 Loss 1.0919\n",
            "Epoch 6 Batch 2150 Loss 1.0887\n",
            "Epoch 6 Batch 2200 Loss 1.0854\n",
            "Epoch 6 Batch 2250 Loss 1.0820\n",
            "Epoch 6 Batch 2300 Loss 1.0787\n",
            "Epoch 6 Batch 2350 Loss 1.0749\n",
            "Epoch 6 Batch 2400 Loss 1.0719\n",
            "Epoch 6 Batch 2450 Loss 1.0689\n",
            "Epoch 6 Batch 2500 Loss 1.0658\n",
            "Epoch 6 Batch 2550 Loss 1.0626\n",
            "Epoch 6 Batch 2600 Loss 1.0598\n",
            "Epoch 6 Batch 2650 Loss 1.0567\n",
            "Epoch 6 Batch 2700 Loss 1.0538\n",
            "Epoch 6 Batch 2750 Loss 1.0506\n",
            "Epoch 6 Batch 2800 Loss 1.0474\n",
            "Epoch 6 Batch 2850 Loss 1.0443\n",
            "Epoch 6 Batch 2900 Loss 1.0414\n",
            "Epoch 6 Batch 2950 Loss 1.0386\n",
            "Epoch 6 Batch 3000 Loss 1.0359\n",
            "Epoch 6 Batch 3050 Loss 1.0336\n",
            "Epoch 6 Batch 3100 Loss 1.0317\n",
            "Epoch 6 Batch 3150 Loss 1.0291\n",
            "Epoch 6 Batch 3200 Loss 1.0270\n",
            "Epoch 6 Batch 3250 Loss 1.0248\n",
            "Epoch 6 Batch 3300 Loss 1.0227\n",
            "Epoch 6 Batch 3350 Loss 1.0208\n",
            "Epoch 6 Batch 3400 Loss 1.0192\n",
            "Epoch 6 Batch 3450 Loss 1.0175\n",
            "Epoch 6 Batch 3500 Loss 1.0159\n",
            "Epoch 6 Batch 3550 Loss 1.0146\n",
            "Epoch 6 Batch 3600 Loss 1.0132\n",
            "Epoch 6 Batch 3650 Loss 1.0119\n",
            "Epoch 6 Batch 3700 Loss 1.0108\n",
            "Epoch 6 Batch 3750 Loss 1.0096\n",
            "Epoch 6 Batch 3800 Loss 1.0083\n",
            "Epoch 6 Batch 3850 Loss 1.0072\n",
            "Epoch 6 Batch 3900 Loss 1.0063\n",
            "Epoch 6 Batch 3950 Loss 1.0052\n",
            "Epoch 6 Batch 4000 Loss 1.0038\n",
            "Epoch 6 Batch 4050 Loss 1.0027\n",
            "Epoch 6 Batch 4100 Loss 1.0018\n",
            "Epoch 6 Batch 4150 Loss 1.0007\n",
            "Epoch 6 Batch 4200 Loss 0.9996\n",
            "Epoch 6 Batch 4250 Loss 0.9984\n",
            "Epoch 6 Batch 4300 Loss 0.9971\n",
            "Epoch 6 Batch 4350 Loss 0.9961\n",
            "Epoch 6 Batch 4400 Loss 0.9950\n",
            "Epoch 6 Batch 4450 Loss 0.9940\n",
            "Epoch 6 Batch 4500 Loss 0.9931\n",
            "Epoch 6 Batch 4550 Loss 0.9924\n",
            "Epoch 6 Batch 4600 Loss 0.9917\n",
            "Epoch 6 Batch 4650 Loss 0.9907\n",
            "Epoch 6 Batch 4700 Loss 0.9898\n",
            "Epoch 6 Batch 4750 Loss 0.9890\n",
            "Epoch 6 Batch 4800 Loss 0.9882\n",
            "Epoch 6 Batch 4850 Loss 0.9875\n",
            "Epoch 6 Batch 4900 Loss 0.9867\n",
            "Epoch 6 Batch 4950 Loss 0.9860\n",
            "Epoch 6 Batch 5000 Loss 0.9849\n",
            "Epoch 6 Batch 5050 Loss 0.9845\n",
            "Epoch 6 Batch 5100 Loss 0.9837\n",
            "Epoch 6 Batch 5150 Loss 0.9828\n",
            "Epoch 6 Batch 5200 Loss 0.9821\n",
            "Epoch 6 Batch 5250 Loss 0.9817\n",
            "Epoch 6 Batch 5300 Loss 0.9810\n",
            "Epoch 6 Batch 5350 Loss 0.9810\n",
            "Epoch 6 Batch 5400 Loss 0.9817\n",
            "Epoch 6 Batch 5450 Loss 0.9826\n",
            "Epoch 6 Batch 5500 Loss 0.9837\n",
            "Epoch 6 Batch 5550 Loss 0.9853\n",
            "Epoch 6 Batch 5600 Loss 0.9870\n",
            "Epoch 6 Batch 5650 Loss 0.9889\n",
            "Epoch 6 Batch 5700 Loss 0.9907\n",
            "Epoch 6 Batch 5750 Loss 0.9924\n",
            "Epoch 6 Batch 5800 Loss 0.9940\n",
            "Epoch 6 Batch 5850 Loss 0.9958\n",
            "Epoch 6 Batch 5900 Loss 0.9973\n",
            "Epoch 6 Batch 5950 Loss 0.9988\n",
            "Epoch 6 Batch 6000 Loss 1.0007\n",
            "Epoch 6 Batch 6050 Loss 1.0024\n",
            "Epoch 6 Batch 6100 Loss 1.0037\n",
            "Epoch 6 Batch 6150 Loss 1.0052\n",
            "Epoch 6 Batch 6200 Loss 1.0069\n",
            "Epoch 6 Batch 6250 Loss 1.0085\n",
            "Epoch 6 Batch 6300 Loss 1.0102\n",
            "Epoch 6 Batch 6350 Loss 1.0119\n",
            "Epoch 6 Batch 6400 Loss 1.0134\n",
            "Epoch 6 Batch 6450 Loss 1.0151\n",
            "Epoch 6 Batch 6500 Loss 1.0167\n",
            "Epoch 6 Batch 6550 Loss 1.0179\n",
            "Epoch 6 Batch 6600 Loss 1.0193\n",
            "Epoch 6 Batch 6650 Loss 1.0205\n",
            "Epoch 6 Batch 6700 Loss 1.0218\n",
            "Epoch 6 Batch 6750 Loss 1.0232\n",
            "Epoch 6 Batch 6800 Loss 1.0244\n",
            "Epoch 6 Batch 6850 Loss 1.0256\n",
            "Epoch 6 Batch 6900 Loss 1.0267\n",
            "Epoch 6 Batch 6950 Loss 1.0278\n",
            "Saving checkpoint for epoch 6 at ./store/ckpt/10hepo/ckpt-6\n",
            "Time taken for 1 epoch: 1744.3531906604767 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.1077\n",
            "Epoch 7 Batch 50 Loss 1.1419\n",
            "Epoch 7 Batch 100 Loss 1.1227\n",
            "Epoch 7 Batch 150 Loss 1.1247\n",
            "Epoch 7 Batch 200 Loss 1.1242\n",
            "Epoch 7 Batch 250 Loss 1.1280\n",
            "Epoch 7 Batch 300 Loss 1.1246\n",
            "Epoch 7 Batch 350 Loss 1.1246\n",
            "Epoch 7 Batch 400 Loss 1.1213\n",
            "Epoch 7 Batch 450 Loss 1.1223\n",
            "Epoch 7 Batch 500 Loss 1.1206\n",
            "Epoch 7 Batch 550 Loss 1.1172\n",
            "Epoch 7 Batch 600 Loss 1.1146\n",
            "Epoch 7 Batch 650 Loss 1.1112\n",
            "Epoch 7 Batch 700 Loss 1.1108\n",
            "Epoch 7 Batch 750 Loss 1.1099\n",
            "Epoch 7 Batch 800 Loss 1.1076\n",
            "Epoch 7 Batch 850 Loss 1.1075\n",
            "Epoch 7 Batch 900 Loss 1.1069\n",
            "Epoch 7 Batch 950 Loss 1.1076\n",
            "Epoch 7 Batch 1000 Loss 1.1088\n",
            "Epoch 7 Batch 1050 Loss 1.1096\n",
            "Epoch 7 Batch 1100 Loss 1.1097\n",
            "Epoch 7 Batch 1150 Loss 1.1097\n",
            "Epoch 7 Batch 1200 Loss 1.1083\n",
            "Epoch 7 Batch 1250 Loss 1.1073\n",
            "Epoch 7 Batch 1300 Loss 1.1070\n",
            "Epoch 7 Batch 1350 Loss 1.1059\n",
            "Epoch 7 Batch 1400 Loss 1.1041\n",
            "Epoch 7 Batch 1450 Loss 1.1019\n",
            "Epoch 7 Batch 1500 Loss 1.1001\n",
            "Epoch 7 Batch 1550 Loss 1.0973\n",
            "Epoch 7 Batch 1600 Loss 1.0943\n",
            "Epoch 7 Batch 1650 Loss 1.0915\n",
            "Epoch 7 Batch 1700 Loss 1.0885\n",
            "Epoch 7 Batch 1750 Loss 1.0853\n",
            "Epoch 7 Batch 1800 Loss 1.0822\n",
            "Epoch 7 Batch 1850 Loss 1.0789\n",
            "Epoch 7 Batch 1900 Loss 1.0753\n",
            "Epoch 7 Batch 1950 Loss 1.0725\n",
            "Epoch 7 Batch 2000 Loss 1.0694\n",
            "Epoch 7 Batch 2050 Loss 1.0661\n",
            "Epoch 7 Batch 2100 Loss 1.0633\n",
            "Epoch 7 Batch 2150 Loss 1.0603\n",
            "Epoch 7 Batch 2200 Loss 1.0572\n",
            "Epoch 7 Batch 2250 Loss 1.0543\n",
            "Epoch 7 Batch 2300 Loss 1.0513\n",
            "Epoch 7 Batch 2350 Loss 1.0481\n",
            "Epoch 7 Batch 2400 Loss 1.0449\n",
            "Epoch 7 Batch 2450 Loss 1.0418\n",
            "Epoch 7 Batch 2500 Loss 1.0388\n",
            "Epoch 7 Batch 2550 Loss 1.0359\n",
            "Epoch 7 Batch 2600 Loss 1.0329\n",
            "Epoch 7 Batch 2650 Loss 1.0301\n",
            "Epoch 7 Batch 2700 Loss 1.0268\n",
            "Epoch 7 Batch 2750 Loss 1.0235\n",
            "Epoch 7 Batch 2800 Loss 1.0199\n",
            "Epoch 7 Batch 2850 Loss 1.0173\n",
            "Epoch 7 Batch 2900 Loss 1.0144\n",
            "Epoch 7 Batch 2950 Loss 1.0117\n",
            "Epoch 7 Batch 3000 Loss 1.0087\n",
            "Epoch 7 Batch 3050 Loss 1.0062\n",
            "Epoch 7 Batch 3100 Loss 1.0038\n",
            "Epoch 7 Batch 3150 Loss 1.0015\n",
            "Epoch 7 Batch 3200 Loss 0.9993\n",
            "Epoch 7 Batch 3250 Loss 0.9973\n",
            "Epoch 7 Batch 3300 Loss 0.9954\n",
            "Epoch 7 Batch 3350 Loss 0.9933\n",
            "Epoch 7 Batch 3400 Loss 0.9914\n",
            "Epoch 7 Batch 3450 Loss 0.9896\n",
            "Epoch 7 Batch 3500 Loss 0.9880\n",
            "Epoch 7 Batch 3550 Loss 0.9868\n",
            "Epoch 7 Batch 3600 Loss 0.9854\n",
            "Epoch 7 Batch 3650 Loss 0.9842\n",
            "Epoch 7 Batch 3700 Loss 0.9831\n",
            "Epoch 7 Batch 3750 Loss 0.9817\n",
            "Epoch 7 Batch 3800 Loss 0.9809\n",
            "Epoch 7 Batch 3850 Loss 0.9800\n",
            "Epoch 7 Batch 3900 Loss 0.9787\n",
            "Epoch 7 Batch 3950 Loss 0.9778\n",
            "Epoch 7 Batch 4000 Loss 0.9767\n",
            "Epoch 7 Batch 4050 Loss 0.9753\n",
            "Epoch 7 Batch 4100 Loss 0.9746\n",
            "Epoch 7 Batch 4150 Loss 0.9738\n",
            "Epoch 7 Batch 4200 Loss 0.9728\n",
            "Epoch 7 Batch 4250 Loss 0.9718\n",
            "Epoch 7 Batch 4300 Loss 0.9707\n",
            "Epoch 7 Batch 4350 Loss 0.9697\n",
            "Epoch 7 Batch 4400 Loss 0.9687\n",
            "Epoch 7 Batch 4450 Loss 0.9678\n",
            "Epoch 7 Batch 4500 Loss 0.9670\n",
            "Epoch 7 Batch 4550 Loss 0.9661\n",
            "Epoch 7 Batch 4600 Loss 0.9653\n",
            "Epoch 7 Batch 4650 Loss 0.9644\n",
            "Epoch 7 Batch 4700 Loss 0.9637\n",
            "Epoch 7 Batch 4750 Loss 0.9630\n",
            "Epoch 7 Batch 4800 Loss 0.9624\n",
            "Epoch 7 Batch 4850 Loss 0.9617\n",
            "Epoch 7 Batch 4900 Loss 0.9610\n",
            "Epoch 7 Batch 4950 Loss 0.9602\n",
            "Epoch 7 Batch 5000 Loss 0.9593\n",
            "Epoch 7 Batch 5050 Loss 0.9587\n",
            "Epoch 7 Batch 5100 Loss 0.9582\n",
            "Epoch 7 Batch 5150 Loss 0.9574\n",
            "Epoch 7 Batch 5200 Loss 0.9565\n",
            "Epoch 7 Batch 5250 Loss 0.9558\n",
            "Epoch 7 Batch 5300 Loss 0.9551\n",
            "Epoch 7 Batch 5350 Loss 0.9550\n",
            "Epoch 7 Batch 5400 Loss 0.9557\n",
            "Epoch 7 Batch 5450 Loss 0.9565\n",
            "Epoch 7 Batch 5500 Loss 0.9579\n",
            "Epoch 7 Batch 5550 Loss 0.9595\n",
            "Epoch 7 Batch 5600 Loss 0.9608\n",
            "Epoch 7 Batch 5650 Loss 0.9627\n",
            "Epoch 7 Batch 5700 Loss 0.9644\n",
            "Epoch 7 Batch 5750 Loss 0.9664\n",
            "Epoch 7 Batch 5800 Loss 0.9679\n",
            "Epoch 7 Batch 5850 Loss 0.9696\n",
            "Epoch 7 Batch 5900 Loss 0.9712\n",
            "Epoch 7 Batch 5950 Loss 0.9726\n",
            "Epoch 7 Batch 6000 Loss 0.9742\n",
            "Epoch 7 Batch 6050 Loss 0.9758\n",
            "Epoch 7 Batch 6100 Loss 0.9773\n",
            "Epoch 7 Batch 6150 Loss 0.9788\n",
            "Epoch 7 Batch 6200 Loss 0.9804\n",
            "Epoch 7 Batch 6250 Loss 0.9821\n",
            "Epoch 7 Batch 6300 Loss 0.9837\n",
            "Epoch 7 Batch 6350 Loss 0.9854\n",
            "Epoch 7 Batch 6400 Loss 0.9871\n",
            "Epoch 7 Batch 6450 Loss 0.9887\n",
            "Epoch 7 Batch 6500 Loss 0.9902\n",
            "Epoch 7 Batch 6550 Loss 0.9916\n",
            "Epoch 7 Batch 6600 Loss 0.9929\n",
            "Epoch 7 Batch 6650 Loss 0.9945\n",
            "Epoch 7 Batch 6700 Loss 0.9958\n",
            "Epoch 7 Batch 6750 Loss 0.9972\n",
            "Epoch 7 Batch 6800 Loss 0.9986\n",
            "Epoch 7 Batch 6850 Loss 0.9996\n",
            "Epoch 7 Batch 6900 Loss 1.0007\n",
            "Epoch 7 Batch 6950 Loss 1.0019\n",
            "Saving checkpoint for epoch 7 at ./store/ckpt/10hepo/ckpt-7\n",
            "Time taken for 1 epoch: 1717.0226981639862 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.2784\n",
            "Epoch 8 Batch 50 Loss 1.1071\n",
            "Epoch 8 Batch 100 Loss 1.1133\n",
            "Epoch 8 Batch 150 Loss 1.1113\n",
            "Epoch 8 Batch 200 Loss 1.1061\n",
            "Epoch 8 Batch 250 Loss 1.1041\n",
            "Epoch 8 Batch 300 Loss 1.0963\n",
            "Epoch 8 Batch 350 Loss 1.0977\n",
            "Epoch 8 Batch 400 Loss 1.0987\n",
            "Epoch 8 Batch 450 Loss 1.0944\n",
            "Epoch 8 Batch 500 Loss 1.0968\n",
            "Epoch 8 Batch 550 Loss 1.0970\n",
            "Epoch 8 Batch 600 Loss 1.0952\n",
            "Epoch 8 Batch 650 Loss 1.0928\n",
            "Epoch 8 Batch 700 Loss 1.0905\n",
            "Epoch 8 Batch 750 Loss 1.0895\n",
            "Epoch 8 Batch 800 Loss 1.0871\n",
            "Epoch 8 Batch 850 Loss 1.0865\n",
            "Epoch 8 Batch 900 Loss 1.0868\n",
            "Epoch 8 Batch 950 Loss 1.0859\n",
            "Epoch 8 Batch 1000 Loss 1.0862\n",
            "Epoch 8 Batch 1050 Loss 1.0861\n",
            "Epoch 8 Batch 1100 Loss 1.0869\n",
            "Epoch 8 Batch 1150 Loss 1.0876\n",
            "Epoch 8 Batch 1200 Loss 1.0873\n",
            "Epoch 8 Batch 1250 Loss 1.0871\n",
            "Epoch 8 Batch 1300 Loss 1.0869\n",
            "Epoch 8 Batch 1350 Loss 1.0852\n",
            "Epoch 8 Batch 1400 Loss 1.0845\n",
            "Epoch 8 Batch 1450 Loss 1.0826\n",
            "Epoch 8 Batch 1500 Loss 1.0794\n",
            "Epoch 8 Batch 1550 Loss 1.0776\n",
            "Epoch 8 Batch 1600 Loss 1.0751\n",
            "Epoch 8 Batch 1650 Loss 1.0713\n",
            "Epoch 8 Batch 1700 Loss 1.0682\n",
            "Epoch 8 Batch 1750 Loss 1.0648\n",
            "Epoch 8 Batch 1800 Loss 1.0616\n",
            "Epoch 8 Batch 1850 Loss 1.0584\n",
            "Epoch 8 Batch 1900 Loss 1.0550\n",
            "Epoch 8 Batch 1950 Loss 1.0513\n",
            "Epoch 8 Batch 2000 Loss 1.0476\n",
            "Epoch 8 Batch 2050 Loss 1.0444\n",
            "Epoch 8 Batch 2100 Loss 1.0406\n",
            "Epoch 8 Batch 2150 Loss 1.0377\n",
            "Epoch 8 Batch 2200 Loss 1.0348\n",
            "Epoch 8 Batch 2250 Loss 1.0316\n",
            "Epoch 8 Batch 2300 Loss 1.0289\n",
            "Epoch 8 Batch 2350 Loss 1.0256\n",
            "Epoch 8 Batch 2400 Loss 1.0224\n",
            "Epoch 8 Batch 2450 Loss 1.0190\n",
            "Epoch 8 Batch 2500 Loss 1.0163\n",
            "Epoch 8 Batch 2550 Loss 1.0135\n",
            "Epoch 8 Batch 2600 Loss 1.0105\n",
            "Epoch 8 Batch 2650 Loss 1.0077\n",
            "Epoch 8 Batch 2700 Loss 1.0046\n",
            "Epoch 8 Batch 2750 Loss 1.0016\n",
            "Epoch 8 Batch 2800 Loss 0.9984\n",
            "Epoch 8 Batch 2850 Loss 0.9953\n",
            "Epoch 8 Batch 2900 Loss 0.9924\n",
            "Epoch 8 Batch 2950 Loss 0.9900\n",
            "Epoch 8 Batch 3000 Loss 0.9873\n",
            "Epoch 8 Batch 3050 Loss 0.9846\n",
            "Epoch 8 Batch 3100 Loss 0.9818\n",
            "Epoch 8 Batch 3150 Loss 0.9798\n",
            "Epoch 8 Batch 3200 Loss 0.9779\n",
            "Epoch 8 Batch 3250 Loss 0.9755\n",
            "Epoch 8 Batch 3300 Loss 0.9737\n",
            "Epoch 8 Batch 3350 Loss 0.9718\n",
            "Epoch 8 Batch 3400 Loss 0.9701\n",
            "Epoch 8 Batch 3450 Loss 0.9685\n",
            "Epoch 8 Batch 3500 Loss 0.9666\n",
            "Epoch 8 Batch 3550 Loss 0.9655\n",
            "Epoch 8 Batch 3600 Loss 0.9644\n",
            "Epoch 8 Batch 3650 Loss 0.9630\n",
            "Epoch 8 Batch 3700 Loss 0.9615\n",
            "Epoch 8 Batch 3750 Loss 0.9604\n",
            "Epoch 8 Batch 3800 Loss 0.9593\n",
            "Epoch 8 Batch 3850 Loss 0.9583\n",
            "Epoch 8 Batch 3900 Loss 0.9574\n",
            "Epoch 8 Batch 3950 Loss 0.9563\n",
            "Epoch 8 Batch 4000 Loss 0.9553\n",
            "Epoch 8 Batch 4050 Loss 0.9543\n",
            "Epoch 8 Batch 4100 Loss 0.9533\n",
            "Epoch 8 Batch 4150 Loss 0.9524\n",
            "Epoch 8 Batch 4200 Loss 0.9515\n",
            "Epoch 8 Batch 4250 Loss 0.9507\n",
            "Epoch 8 Batch 4300 Loss 0.9497\n",
            "Epoch 8 Batch 4350 Loss 0.9489\n",
            "Epoch 8 Batch 4400 Loss 0.9481\n",
            "Epoch 8 Batch 4450 Loss 0.9470\n",
            "Epoch 8 Batch 4500 Loss 0.9463\n",
            "Epoch 8 Batch 4550 Loss 0.9456\n",
            "Epoch 8 Batch 4600 Loss 0.9444\n",
            "Epoch 8 Batch 4650 Loss 0.9436\n",
            "Epoch 8 Batch 4700 Loss 0.9429\n",
            "Epoch 8 Batch 4750 Loss 0.9421\n",
            "Epoch 8 Batch 4800 Loss 0.9415\n",
            "Epoch 8 Batch 4850 Loss 0.9407\n",
            "Epoch 8 Batch 4900 Loss 0.9398\n",
            "Epoch 8 Batch 4950 Loss 0.9391\n",
            "Epoch 8 Batch 5000 Loss 0.9386\n",
            "Epoch 8 Batch 5050 Loss 0.9378\n",
            "Epoch 8 Batch 5100 Loss 0.9372\n",
            "Epoch 8 Batch 5150 Loss 0.9364\n",
            "Epoch 8 Batch 5200 Loss 0.9358\n",
            "Epoch 8 Batch 5250 Loss 0.9352\n",
            "Epoch 8 Batch 5300 Loss 0.9343\n",
            "Epoch 8 Batch 5350 Loss 0.9342\n",
            "Epoch 8 Batch 5400 Loss 0.9346\n",
            "Epoch 8 Batch 5450 Loss 0.9356\n",
            "Epoch 8 Batch 5500 Loss 0.9368\n",
            "Epoch 8 Batch 5550 Loss 0.9384\n",
            "Epoch 8 Batch 5600 Loss 0.9402\n",
            "Epoch 8 Batch 5650 Loss 0.9419\n",
            "Epoch 8 Batch 5700 Loss 0.9438\n",
            "Epoch 8 Batch 5750 Loss 0.9455\n",
            "Epoch 8 Batch 5800 Loss 0.9473\n",
            "Epoch 8 Batch 5850 Loss 0.9491\n",
            "Epoch 8 Batch 5900 Loss 0.9508\n",
            "Epoch 8 Batch 5950 Loss 0.9522\n",
            "Epoch 8 Batch 6000 Loss 0.9538\n",
            "Epoch 8 Batch 6050 Loss 0.9553\n",
            "Epoch 8 Batch 6100 Loss 0.9571\n",
            "Epoch 8 Batch 6150 Loss 0.9586\n",
            "Epoch 8 Batch 6200 Loss 0.9602\n",
            "Epoch 8 Batch 6250 Loss 0.9616\n",
            "Epoch 8 Batch 6300 Loss 0.9634\n",
            "Epoch 8 Batch 6350 Loss 0.9653\n",
            "Epoch 8 Batch 6400 Loss 0.9668\n",
            "Epoch 8 Batch 6450 Loss 0.9685\n",
            "Epoch 8 Batch 6500 Loss 0.9700\n",
            "Epoch 8 Batch 6550 Loss 0.9715\n",
            "Epoch 8 Batch 6600 Loss 0.9731\n",
            "Epoch 8 Batch 6650 Loss 0.9747\n",
            "Epoch 8 Batch 6700 Loss 0.9761\n",
            "Epoch 8 Batch 6750 Loss 0.9775\n",
            "Epoch 8 Batch 6800 Loss 0.9785\n",
            "Epoch 8 Batch 6850 Loss 0.9795\n",
            "Epoch 8 Batch 6900 Loss 0.9808\n",
            "Epoch 8 Batch 6950 Loss 0.9818\n",
            "Saving checkpoint for epoch 8 at ./store/ckpt/10hepo/ckpt-8\n",
            "Time taken for 1 epoch: 1718.5755999088287 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.1424\n",
            "Epoch 9 Batch 50 Loss 1.1009\n",
            "Epoch 9 Batch 100 Loss 1.1117\n",
            "Epoch 9 Batch 150 Loss 1.1008\n",
            "Epoch 9 Batch 200 Loss 1.0979\n",
            "Epoch 9 Batch 250 Loss 1.0907\n",
            "Epoch 9 Batch 300 Loss 1.0896\n",
            "Epoch 9 Batch 350 Loss 1.0848\n",
            "Epoch 9 Batch 400 Loss 1.0827\n",
            "Epoch 9 Batch 450 Loss 1.0824\n",
            "Epoch 9 Batch 500 Loss 1.0787\n",
            "Epoch 9 Batch 550 Loss 1.0772\n",
            "Epoch 9 Batch 600 Loss 1.0769\n",
            "Epoch 9 Batch 650 Loss 1.0784\n",
            "Epoch 9 Batch 700 Loss 1.0777\n",
            "Epoch 9 Batch 750 Loss 1.0739\n",
            "Epoch 9 Batch 800 Loss 1.0725\n",
            "Epoch 9 Batch 850 Loss 1.0704\n",
            "Epoch 9 Batch 900 Loss 1.0696\n",
            "Epoch 9 Batch 950 Loss 1.0709\n",
            "Epoch 9 Batch 1000 Loss 1.0705\n",
            "Epoch 9 Batch 1050 Loss 1.0699\n",
            "Epoch 9 Batch 1100 Loss 1.0693\n",
            "Epoch 9 Batch 1150 Loss 1.0706\n",
            "Epoch 9 Batch 1200 Loss 1.0708\n",
            "Epoch 9 Batch 1250 Loss 1.0696\n",
            "Epoch 9 Batch 1300 Loss 1.0691\n",
            "Epoch 9 Batch 1350 Loss 1.0686\n",
            "Epoch 9 Batch 1400 Loss 1.0671\n",
            "Epoch 9 Batch 1450 Loss 1.0645\n",
            "Epoch 9 Batch 1500 Loss 1.0613\n",
            "Epoch 9 Batch 1550 Loss 1.0581\n",
            "Epoch 9 Batch 1600 Loss 1.0555\n",
            "Epoch 9 Batch 1650 Loss 1.0524\n",
            "Epoch 9 Batch 1700 Loss 1.0492\n",
            "Epoch 9 Batch 1750 Loss 1.0458\n",
            "Epoch 9 Batch 1800 Loss 1.0425\n",
            "Epoch 9 Batch 1850 Loss 1.0394\n",
            "Epoch 9 Batch 1900 Loss 1.0366\n",
            "Epoch 9 Batch 1950 Loss 1.0337\n",
            "Epoch 9 Batch 2000 Loss 1.0305\n",
            "Epoch 9 Batch 2050 Loss 1.0271\n",
            "Epoch 9 Batch 2100 Loss 1.0243\n",
            "Epoch 9 Batch 2150 Loss 1.0214\n",
            "Epoch 9 Batch 2200 Loss 1.0181\n",
            "Epoch 9 Batch 2250 Loss 1.0144\n",
            "Epoch 9 Batch 2300 Loss 1.0113\n",
            "Epoch 9 Batch 2350 Loss 1.0079\n",
            "Epoch 9 Batch 2400 Loss 1.0044\n",
            "Epoch 9 Batch 2450 Loss 1.0012\n",
            "Epoch 9 Batch 2500 Loss 0.9980\n",
            "Epoch 9 Batch 2550 Loss 0.9958\n",
            "Epoch 9 Batch 2600 Loss 0.9929\n",
            "Epoch 9 Batch 2650 Loss 0.9901\n",
            "Epoch 9 Batch 2700 Loss 0.9872\n",
            "Epoch 9 Batch 2750 Loss 0.9841\n",
            "Epoch 9 Batch 2800 Loss 0.9811\n",
            "Epoch 9 Batch 2850 Loss 0.9780\n",
            "Epoch 9 Batch 2900 Loss 0.9753\n",
            "Epoch 9 Batch 2950 Loss 0.9728\n",
            "Epoch 9 Batch 3000 Loss 0.9703\n",
            "Epoch 9 Batch 3050 Loss 0.9680\n",
            "Epoch 9 Batch 3100 Loss 0.9650\n",
            "Epoch 9 Batch 3150 Loss 0.9632\n",
            "Epoch 9 Batch 3200 Loss 0.9610\n",
            "Epoch 9 Batch 3250 Loss 0.9590\n",
            "Epoch 9 Batch 3300 Loss 0.9570\n",
            "Epoch 9 Batch 3350 Loss 0.9551\n",
            "Epoch 9 Batch 3400 Loss 0.9534\n",
            "Epoch 9 Batch 3450 Loss 0.9518\n",
            "Epoch 9 Batch 3500 Loss 0.9503\n",
            "Epoch 9 Batch 3550 Loss 0.9491\n",
            "Epoch 9 Batch 3600 Loss 0.9479\n",
            "Epoch 9 Batch 3650 Loss 0.9467\n",
            "Epoch 9 Batch 3700 Loss 0.9454\n",
            "Epoch 9 Batch 3750 Loss 0.9444\n",
            "Epoch 9 Batch 3800 Loss 0.9433\n",
            "Epoch 9 Batch 3850 Loss 0.9425\n",
            "Epoch 9 Batch 3900 Loss 0.9413\n",
            "Epoch 9 Batch 3950 Loss 0.9402\n",
            "Epoch 9 Batch 4000 Loss 0.9389\n",
            "Epoch 9 Batch 4050 Loss 0.9378\n",
            "Epoch 9 Batch 4100 Loss 0.9369\n",
            "Epoch 9 Batch 4150 Loss 0.9360\n",
            "Epoch 9 Batch 4200 Loss 0.9348\n",
            "Epoch 9 Batch 4250 Loss 0.9341\n",
            "Epoch 9 Batch 4300 Loss 0.9331\n",
            "Epoch 9 Batch 4350 Loss 0.9325\n",
            "Epoch 9 Batch 4400 Loss 0.9315\n",
            "Epoch 9 Batch 4450 Loss 0.9306\n",
            "Epoch 9 Batch 4500 Loss 0.9298\n",
            "Epoch 9 Batch 4550 Loss 0.9291\n",
            "Epoch 9 Batch 4600 Loss 0.9282\n",
            "Epoch 9 Batch 4650 Loss 0.9273\n",
            "Epoch 9 Batch 4700 Loss 0.9265\n",
            "Epoch 9 Batch 4750 Loss 0.9256\n",
            "Epoch 9 Batch 4800 Loss 0.9246\n",
            "Epoch 9 Batch 4850 Loss 0.9238\n",
            "Epoch 9 Batch 4900 Loss 0.9233\n",
            "Epoch 9 Batch 4950 Loss 0.9224\n",
            "Epoch 9 Batch 5000 Loss 0.9218\n",
            "Epoch 9 Batch 5050 Loss 0.9210\n",
            "Epoch 9 Batch 5100 Loss 0.9203\n",
            "Epoch 9 Batch 5150 Loss 0.9196\n",
            "Epoch 9 Batch 5200 Loss 0.9188\n",
            "Epoch 9 Batch 5250 Loss 0.9182\n",
            "Epoch 9 Batch 5300 Loss 0.9177\n",
            "Epoch 9 Batch 5350 Loss 0.9178\n",
            "Epoch 9 Batch 5400 Loss 0.9186\n",
            "Epoch 9 Batch 5450 Loss 0.9194\n",
            "Epoch 9 Batch 5500 Loss 0.9204\n",
            "Epoch 9 Batch 5550 Loss 0.9218\n",
            "Epoch 9 Batch 5600 Loss 0.9235\n",
            "Epoch 9 Batch 5650 Loss 0.9251\n",
            "Epoch 9 Batch 5700 Loss 0.9268\n",
            "Epoch 9 Batch 5750 Loss 0.9287\n",
            "Epoch 9 Batch 5800 Loss 0.9304\n",
            "Epoch 9 Batch 5850 Loss 0.9322\n",
            "Epoch 9 Batch 5900 Loss 0.9340\n",
            "Epoch 9 Batch 5950 Loss 0.9355\n",
            "Epoch 9 Batch 6000 Loss 0.9374\n",
            "Epoch 9 Batch 6050 Loss 0.9390\n",
            "Epoch 9 Batch 6100 Loss 0.9405\n",
            "Epoch 9 Batch 6150 Loss 0.9422\n",
            "Epoch 9 Batch 6200 Loss 0.9438\n",
            "Epoch 9 Batch 6250 Loss 0.9453\n",
            "Epoch 9 Batch 6300 Loss 0.9469\n",
            "Epoch 9 Batch 6350 Loss 0.9489\n",
            "Epoch 9 Batch 6400 Loss 0.9505\n",
            "Epoch 9 Batch 6450 Loss 0.9522\n",
            "Epoch 9 Batch 6500 Loss 0.9536\n",
            "Epoch 9 Batch 6550 Loss 0.9552\n",
            "Epoch 9 Batch 6600 Loss 0.9566\n",
            "Epoch 9 Batch 6650 Loss 0.9579\n",
            "Epoch 9 Batch 6700 Loss 0.9592\n",
            "Epoch 9 Batch 6750 Loss 0.9604\n",
            "Epoch 9 Batch 6800 Loss 0.9617\n",
            "Epoch 9 Batch 6850 Loss 0.9628\n",
            "Epoch 9 Batch 6900 Loss 0.9640\n",
            "Epoch 9 Batch 6950 Loss 0.9652\n",
            "Saving checkpoint for epoch 9 at ./store/ckpt/10hepo/ckpt-9\n",
            "Time taken for 1 epoch: 1724.875305891037 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.2200\n",
            "Epoch 10 Batch 50 Loss 1.0870\n",
            "Epoch 10 Batch 100 Loss 1.0818\n",
            "Epoch 10 Batch 150 Loss 1.0837\n",
            "Epoch 10 Batch 200 Loss 1.0820\n",
            "Epoch 10 Batch 250 Loss 1.0803\n",
            "Epoch 10 Batch 300 Loss 1.0769\n",
            "Epoch 10 Batch 350 Loss 1.0721\n",
            "Epoch 10 Batch 400 Loss 1.0711\n",
            "Epoch 10 Batch 450 Loss 1.0693\n",
            "Epoch 10 Batch 500 Loss 1.0665\n",
            "Epoch 10 Batch 550 Loss 1.0635\n",
            "Epoch 10 Batch 600 Loss 1.0624\n",
            "Epoch 10 Batch 650 Loss 1.0612\n",
            "Epoch 10 Batch 700 Loss 1.0591\n",
            "Epoch 10 Batch 750 Loss 1.0580\n",
            "Epoch 10 Batch 800 Loss 1.0564\n",
            "Epoch 10 Batch 850 Loss 1.0567\n",
            "Epoch 10 Batch 900 Loss 1.0564\n",
            "Epoch 10 Batch 950 Loss 1.0550\n",
            "Epoch 10 Batch 1000 Loss 1.0558\n",
            "Epoch 10 Batch 1050 Loss 1.0568\n",
            "Epoch 10 Batch 1100 Loss 1.0579\n",
            "Epoch 10 Batch 1150 Loss 1.0580\n",
            "Epoch 10 Batch 1200 Loss 1.0582\n",
            "Epoch 10 Batch 1250 Loss 1.0570\n",
            "Epoch 10 Batch 1300 Loss 1.0566\n",
            "Epoch 10 Batch 1350 Loss 1.0552\n",
            "Epoch 10 Batch 1400 Loss 1.0531\n",
            "Epoch 10 Batch 1450 Loss 1.0509\n",
            "Epoch 10 Batch 1500 Loss 1.0489\n",
            "Epoch 10 Batch 1550 Loss 1.0462\n",
            "Epoch 10 Batch 1600 Loss 1.0432\n",
            "Epoch 10 Batch 1650 Loss 1.0401\n",
            "Epoch 10 Batch 1700 Loss 1.0369\n",
            "Epoch 10 Batch 1750 Loss 1.0335\n",
            "Epoch 10 Batch 1800 Loss 1.0299\n",
            "Epoch 10 Batch 1850 Loss 1.0260\n",
            "Epoch 10 Batch 1900 Loss 1.0229\n",
            "Epoch 10 Batch 1950 Loss 1.0194\n",
            "Epoch 10 Batch 2000 Loss 1.0164\n",
            "Epoch 10 Batch 2050 Loss 1.0130\n",
            "Epoch 10 Batch 2100 Loss 1.0098\n",
            "Epoch 10 Batch 2150 Loss 1.0066\n",
            "Epoch 10 Batch 2200 Loss 1.0028\n",
            "Epoch 10 Batch 2250 Loss 0.9993\n",
            "Epoch 10 Batch 2300 Loss 0.9962\n",
            "Epoch 10 Batch 2350 Loss 0.9930\n",
            "Epoch 10 Batch 2400 Loss 0.9899\n",
            "Epoch 10 Batch 2450 Loss 0.9871\n",
            "Epoch 10 Batch 2500 Loss 0.9843\n",
            "Epoch 10 Batch 2550 Loss 0.9818\n",
            "Epoch 10 Batch 2600 Loss 0.9788\n",
            "Epoch 10 Batch 2650 Loss 0.9760\n",
            "Epoch 10 Batch 2700 Loss 0.9729\n",
            "Epoch 10 Batch 2750 Loss 0.9693\n",
            "Epoch 10 Batch 2800 Loss 0.9662\n",
            "Epoch 10 Batch 2850 Loss 0.9631\n",
            "Epoch 10 Batch 2900 Loss 0.9608\n",
            "Epoch 10 Batch 2950 Loss 0.9583\n",
            "Epoch 10 Batch 3000 Loss 0.9560\n",
            "Epoch 10 Batch 3050 Loss 0.9535\n",
            "Epoch 10 Batch 3100 Loss 0.9514\n",
            "Epoch 10 Batch 3150 Loss 0.9489\n",
            "Epoch 10 Batch 3200 Loss 0.9467\n",
            "Epoch 10 Batch 3250 Loss 0.9445\n",
            "Epoch 10 Batch 3300 Loss 0.9425\n",
            "Epoch 10 Batch 3350 Loss 0.9408\n",
            "Epoch 10 Batch 3400 Loss 0.9393\n",
            "Epoch 10 Batch 3450 Loss 0.9377\n",
            "Epoch 10 Batch 3500 Loss 0.9361\n",
            "Epoch 10 Batch 3550 Loss 0.9348\n",
            "Epoch 10 Batch 3600 Loss 0.9334\n",
            "Epoch 10 Batch 3650 Loss 0.9321\n",
            "Epoch 10 Batch 3700 Loss 0.9307\n",
            "Epoch 10 Batch 3750 Loss 0.9295\n",
            "Epoch 10 Batch 3800 Loss 0.9284\n",
            "Epoch 10 Batch 3850 Loss 0.9275\n",
            "Epoch 10 Batch 3900 Loss 0.9264\n",
            "Epoch 10 Batch 3950 Loss 0.9255\n",
            "Epoch 10 Batch 4000 Loss 0.9244\n",
            "Epoch 10 Batch 4050 Loss 0.9237\n",
            "Epoch 10 Batch 4100 Loss 0.9226\n",
            "Epoch 10 Batch 4150 Loss 0.9217\n",
            "Epoch 10 Batch 4200 Loss 0.9207\n",
            "Epoch 10 Batch 4250 Loss 0.9196\n",
            "Epoch 10 Batch 4300 Loss 0.9186\n",
            "Epoch 10 Batch 4350 Loss 0.9176\n",
            "Epoch 10 Batch 4400 Loss 0.9169\n",
            "Epoch 10 Batch 4450 Loss 0.9160\n",
            "Epoch 10 Batch 4500 Loss 0.9151\n",
            "Epoch 10 Batch 4550 Loss 0.9142\n",
            "Epoch 10 Batch 4600 Loss 0.9134\n",
            "Epoch 10 Batch 4650 Loss 0.9126\n",
            "Epoch 10 Batch 4700 Loss 0.9116\n",
            "Epoch 10 Batch 4750 Loss 0.9109\n",
            "Epoch 10 Batch 4800 Loss 0.9100\n",
            "Epoch 10 Batch 4850 Loss 0.9095\n",
            "Epoch 10 Batch 4900 Loss 0.9086\n",
            "Epoch 10 Batch 4950 Loss 0.9081\n",
            "Epoch 10 Batch 5000 Loss 0.9076\n",
            "Epoch 10 Batch 5050 Loss 0.9069\n",
            "Epoch 10 Batch 5100 Loss 0.9064\n",
            "Epoch 10 Batch 5150 Loss 0.9056\n",
            "Epoch 10 Batch 5200 Loss 0.9050\n",
            "Epoch 10 Batch 5250 Loss 0.9043\n",
            "Epoch 10 Batch 5300 Loss 0.9035\n",
            "Epoch 10 Batch 5350 Loss 0.9036\n",
            "Epoch 10 Batch 5400 Loss 0.9042\n",
            "Epoch 10 Batch 5450 Loss 0.9053\n",
            "Epoch 10 Batch 5500 Loss 0.9063\n",
            "Epoch 10 Batch 5550 Loss 0.9079\n",
            "Epoch 10 Batch 5600 Loss 0.9097\n",
            "Epoch 10 Batch 5650 Loss 0.9115\n",
            "Epoch 10 Batch 5700 Loss 0.9132\n",
            "Epoch 10 Batch 5750 Loss 0.9149\n",
            "Epoch 10 Batch 5800 Loss 0.9168\n",
            "Epoch 10 Batch 5850 Loss 0.9186\n",
            "Epoch 10 Batch 5900 Loss 0.9203\n",
            "Epoch 10 Batch 5950 Loss 0.9219\n",
            "Epoch 10 Batch 6000 Loss 0.9233\n",
            "Epoch 10 Batch 6050 Loss 0.9249\n",
            "Epoch 10 Batch 6100 Loss 0.9264\n",
            "Epoch 10 Batch 6150 Loss 0.9279\n",
            "Epoch 10 Batch 6200 Loss 0.9295\n",
            "Epoch 10 Batch 6250 Loss 0.9312\n",
            "Epoch 10 Batch 6300 Loss 0.9329\n",
            "Epoch 10 Batch 6350 Loss 0.9345\n",
            "Epoch 10 Batch 6400 Loss 0.9362\n",
            "Epoch 10 Batch 6450 Loss 0.9379\n",
            "Epoch 10 Batch 6500 Loss 0.9394\n",
            "Epoch 10 Batch 6550 Loss 0.9411\n",
            "Epoch 10 Batch 6600 Loss 0.9425\n",
            "Epoch 10 Batch 6650 Loss 0.9439\n",
            "Epoch 10 Batch 6700 Loss 0.9452\n",
            "Epoch 10 Batch 6750 Loss 0.9464\n",
            "Epoch 10 Batch 6800 Loss 0.9476\n",
            "Epoch 10 Batch 6850 Loss 0.9489\n",
            "Epoch 10 Batch 6900 Loss 0.9502\n",
            "Epoch 10 Batch 6950 Loss 0.9515\n",
            "Saving checkpoint for epoch 10 at ./store/ckpt/10hepo/ckpt-10\n",
            "Time taken for 1 epoch: 1726.9825325012207 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlYEAUjgxGVS"
      },
      "source": [
        "**Evaluate Model Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fXAuSzwzxFO9"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_SV-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions, weights = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_SV-1:\n",
        "            return tf.squeeze(output, axis=0), weights\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0), weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l2l5dUKBxjuA"
      },
      "source": [
        "def translate(sentence):\n",
        "    output, weights = evaluate(sentence)\n",
        "    \n",
        "    output = output.numpy()\n",
        "    predicted_sentence = tokenizer_sv.decode([i for i in output if i < VOCAB_SIZE_SV-2])\n",
        "    \n",
        "    #print(\"Input: {}\".format(sentence))\n",
        "    #print(\"Predicted translation: {}\".format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U3xAXf0dx11L",
        "outputId": "9632efc5-355e-42ba-fc0d-cf1123276dda"
      },
      "source": [
        "result, a = translate(\"I congratulate him on his excellent report.\")\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jag gratulerar honom till hans utmärkta betänkande.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4oWtZ-UcK9lC",
        "outputId": "8d11f7c9-610e-4a2d-fd29-786dbaa7c2d2"
      },
      "source": [
        "a = tf.squeeze(a, 0)\n",
        "a = a[0]\n",
        "a.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p7VBfs67y1cx",
        "outputId": "6f7e96e0-8652-4888-f7ac-4310f7a2cce0"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dooB2rtEy-J_"
      },
      "source": [
        "import sacrebleu\n",
        "avg = 0\n",
        "total = 0\n",
        "\n",
        "for j in range(len(valid_src)):\n",
        "  src = tokenizer_en.decode([i for i in valid_src[j] if i < VOCAB_SIZE_EN-2])\n",
        "  ref = tokenizer_sv.decode([i for i in valid_ref[j] if i < VOCAB_SIZE_SV-2])\n",
        "  if len(src.split()) < 1 or len(ref.split()) < 1:\n",
        "    continue\n",
        "  total += 1\n",
        "  translated, _ = translate(src)\n",
        "  avg += sacrebleu.raw_corpus_bleu(translated, ref, 0.01).score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXUXw42Eo_zV"
      },
      "source": [
        "**We obtain an impressive BLEU score of 22.11 on the validation dataset!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkUZaIveIJYN"
      },
      "source": [
        "print('BLEU SCORE: ', round(avg/len(valid_src), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDIdY5NJ9ug"
      },
      "source": [
        "###Test case for attention plot###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention(attention):\n",
        "\n",
        "  translation, attention = translate(sentence)\n",
        "\n",
        "  attention = tf.squeeze(attention, 0)\n",
        "  attention = attention[0]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(sentence.split())))\n",
        "  ax.set_yticks(range(len(translation.split())))\n",
        "\n",
        "  ax.set_xticklabels(sentence.split(), rotation=90)\n",
        "  ax.set_yticklabels(translation.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR-eEtAesRyP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "def plot_attention(sentence):\n",
        "  \n",
        "    translation, attention = translate(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention, 0)\n",
        "    attention = attention[0]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(attention, cmap='GnBu_r')\n",
        "\n",
        "    ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
        "    ax.set_yticklabels(translation.split(), minor=False)\n",
        "\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(attention.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(attention.shape[0]) + 0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_UDub_RAeYj"
      },
      "source": [
        "sentence = 'Are there any comments?'\n",
        "plot_attention(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y42cJlPt1ss"
      },
      "source": [
        "sentence = 'Safety advisers for the transport of dangerous goods'\n",
        "plot_attention(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}